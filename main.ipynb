{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T08:46:21.105992Z",
     "iopub.status.busy": "2022-06-15T08:46:21.105270Z",
     "iopub.status.idle": "2022-06-15T08:47:08.896577Z",
     "shell.execute_reply": "2022-06-15T08:47:08.895473Z",
     "shell.execute_reply.started": "2022-06-15T08:46:21.105949Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting lifelines==0.27.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8f/f6/56817d77b6a825604bf4eed0a27995a77a6ba3ee5f57d6baa1aacf77d62b/lifelines-0.27.0-py3-none-any.whl (349 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.1/349.1 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from lifelines==0.27.0) (1.6.3)\n",
      "Collecting autograd>=1.3\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d9/6e/5aec16d68bf07e17e1a6cac5011e1c8f5f8dadb0ac5e875d432ee8aaa733/autograd-1.4-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from lifelines==0.27.0) (1.1.5)\n",
      "Collecting formulaic>=0.2.2\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1a/ae/c3ce1d2935abddef895fd6b0e4f1a06c8d5ff92f6f7ed87861f6e1616df2/formulaic-0.3.4-py3-none-any.whl (68 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.2/68.2 KB\u001b[0m \u001b[31m253.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting autograd-gamma>=0.3\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/85/ae/7f2031ea76140444b2453fa139041e5afd4a09fc5300cfefeb1103291f80/autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from lifelines==0.27.0) (1.20.3)\n",
      "Collecting matplotlib>=3.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/df/e7/0ad4aad00d6d0314aaf97526a54a34d385f898ebb7915d112431fff452ff/matplotlib-3.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: future>=0.15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from autograd>=1.3->lifelines==0.27.0) (0.18.0)\n",
      "Collecting pandas>=1.0.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3e/0c/23764c4635dcb0a784a787498d56847b90ebf974e65f4ab4053a5d97b1a5/pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: astor>=0.8 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from formulaic>=0.2.2->lifelines==0.27.0) (0.8.1)\n",
      "Requirement already satisfied: wrapt>=1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from formulaic>=0.2.2->lifelines==0.27.0) (1.12.1)\n",
      "Collecting interface-meta<2.0.0,>=1.2.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/02/3f/a6ec28c88e2d8e54d32598a1e0b5208a4baa72a8e7f6e241beab5731eb9d/interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib>=3.0->lifelines==0.27.0) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib>=3.0->lifelines==0.27.0) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib>=3.0->lifelines==0.27.0) (21.3)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2f/85/2f6e42fb4b537b9998835410578fb1973175b81691e9a82ab6668cf64b0b/fonttools-4.33.3-py3-none-any.whl (930 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.9/930.9 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib>=3.0->lifelines==0.27.0) (3.0.8)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib>=3.0->lifelines==0.27.0) (7.1.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib>=3.0->lifelines==0.27.0) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas>=1.0.0->lifelines==0.27.0) (2019.3)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=3.0->lifelines==0.27.0) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=3.0->lifelines==0.27.0) (56.2.0)\n",
      "Building wheels for collected packages: autograd-gamma\n",
      "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4034 sha256=bee6e63c990949d94e988f30851e394c2f6b7c373078c5276a4eff3823268fad\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/ab/0b/5c/977e05dfabbeeb83fc49bdb43b17fbdf47a3dfc2913a4d4a27\n",
      "Successfully built autograd-gamma\n",
      "Installing collected packages: interface-meta, fonttools, autograd, pandas, matplotlib, autograd-gamma, formulaic, lifelines\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.1.5\n",
      "    Uninstalling pandas-1.1.5:\n",
      "      Successfully uninstalled pandas-1.1.5\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 2.2.3\n",
      "    Uninstalling matplotlib-2.2.3:\n",
      "      Successfully uninstalled matplotlib-2.2.3\n",
      "Successfully installed autograd-1.4 autograd-gamma-0.5.0 fonttools-4.33.3 formulaic-0.3.4 interface-meta-1.3.0 lifelines-0.27.0 matplotlib-3.5.2 pandas-1.3.5\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/envs/python35-paddle120-env/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mUsing pip 22.0.4 from /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip (python 3.7)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "  Link requires a different Python (3.7.4 not in: '>=3.8'): https://pypi.tuna.tsinghua.edu.cn/packages/4d/aa/e7078569d20f45e8cf6512a24bf2945698f13a7975650773c01366ea96dc/pandas-1.4.0.tar.gz#sha256=cdd76254c7f0a1583bd4e4781fb450d0ebf392e10d3f12e92c95575942e37df5 (from https://pypi.tuna.tsinghua.edu.cn/simple/pandas/) (requires-python:>=3.8)\n",
      "  Link requires a different Python (3.7.4 not in: '>=3.8'): https://pypi.tuna.tsinghua.edu.cn/packages/29/e3/6bd596d81eaf9f5b35398fdac0c535efadd9bbf8d0f859739badf9f90c63/pandas-1.4.0rc0.tar.gz#sha256=c0d453fda0a87d51f5fe65c16a89b64f13a736f4f17c0202cfcff67e6b341a57 (from https://pypi.tuna.tsinghua.edu.cn/simple/pandas/) (requires-python:>=3.8)\n",
      "  Link requires a different Python (3.7.4 not in: '>=3.8'): https://pypi.tuna.tsinghua.edu.cn/packages/c4/eb/cfa96ba42695b3c28d4864a796d492f188471dd536df7e5e5e0c54b629a6/pandas-1.4.1.tar.gz#sha256=8db93ec98ac7cb5f8ac1420c10f5e3c43533153f253fe7fb6d891cf5aa2b80d2 (from https://pypi.tuna.tsinghua.edu.cn/simple/pandas/) (requires-python:>=3.8)\n",
      "  Link requires a different Python (3.7.4 not in: '>=3.8'): https://pypi.tuna.tsinghua.edu.cn/packages/5a/ac/b3b9aa2318de52e40c26ae7b9ce6d4e9d1bcdaf5da0899a691642117cf60/pandas-1.4.2.tar.gz#sha256=92bc1fc585f1463ca827b45535957815b7deb218c549b7c18402c322c7549a12 (from https://pypi.tuna.tsinghua.edu.cn/simple/pandas/) (requires-python:>=3.8)\n",
      "Collecting pandas==1.2.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ff/bd/fb376f9fbad92b9a6efdbb30ff32c80f3cba1368689309cbb5566364af5c/pandas-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas==1.2.0) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas==1.2.0) (1.20.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas==1.2.0) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.2.0) (1.16.0)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.5\n",
      "    Uninstalling pandas-1.3.5:\n",
      "      Removing file or directory /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pandas-1.3.5.dist-info/\n",
      "      Removing file or directory /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pandas/\n",
      "      Successfully uninstalled pandas-1.3.5\n",
      "Successfully installed pandas-1.2.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/envs/python35-paddle120-env/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mUsing pip 22.0.4 from /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pip (python 3.7)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting matplotlib==2.2.3\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/52/46/ff47fea8e5c528c497fc385c95887131c4319a3411814ba9a766b66a9367/matplotlib-2.2.3-cp37-cp37m-manylinux1_x86_64.whl (12.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib==2.2.3) (1.16.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib==2.2.3) (2019.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib==2.2.3) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib==2.2.3) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib==2.2.3) (3.0.8)\n",
      "Requirement already satisfied: numpy>=1.7.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib==2.2.3) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib==2.2.3) (2.8.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib==2.2.3) (56.2.0)\n",
      "Installing collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.5.2\n",
      "    Uninstalling matplotlib-3.5.2:\n",
      "      Removing file or directory /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/__pycache__/pylab.cpython-37.pyc\n",
      "      Removing file or directory /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib-3.5.2-py3.7-nspkg.pth\n",
      "      Removing file or directory /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib-3.5.2.dist-info/\n",
      "      Removing file or directory /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/\n",
      "      Removing file or directory /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/mpl_toolkits/axes_grid/\n",
      "      Removing file or directory /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/mpl_toolkits/axes_grid1/\n",
      "      Removing file or directory /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/mpl_toolkits/axisartist/\n",
      "      Removing file or directory /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/mpl_toolkits/mplot3d/\n",
      "      Removing file or directory /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/mpl_toolkits/tests/\n",
      "      Removing file or directory /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pylab.py\n",
      "      Successfully uninstalled matplotlib-3.5.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "lifelines 0.27.0 requires matplotlib>=3.0, but you have matplotlib 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed matplotlib-2.2.3\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/envs/python35-paddle120-env/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting pgl\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6d/2d/8f3dd46371f4c02d71c2d41f2fe64a063a53bf232466df011fef399c16c8/pgl-2.2.3.post0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy<1.20.0,>=1.16.4\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/08/d6/a6aaa29fea945bc6c61d11f6e0697b325ff7446de5ffd62c2fa02f627048/numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cython>=0.25.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pgl) (0.29)\n",
      "Installing collected packages: numpy, pgl\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.3\n",
      "    Uninstalling numpy-1.20.3:\n",
      "      Successfully uninstalled numpy-1.20.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "parl 1.4.1 requires pyzmq==18.1.1, but you have pyzmq 22.3.0 which is incompatible.\n",
      "lifelines 0.27.0 requires matplotlib>=3.0, but you have matplotlib 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.19.5 pgl-2.2.3.post0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/envs/python35-paddle120-env/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -V lifelines==0.27.0\n",
    "! pip install -v pandas==1.2.0\n",
    "! pip install -v matplotlib==2.2.3\n",
    "! pip install pgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T09:47:12.810601Z",
     "iopub.status.busy": "2022-06-15T09:47:12.810361Z",
     "iopub.status.idle": "2022-06-15T09:47:14.855123Z",
     "shell.execute_reply": "2022-06-15T09:47:14.854459Z",
     "shell.execute_reply.started": "2022-06-15T09:47:12.810576Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import MutableMapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable, Mapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sized\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "import pgl\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "from sklearn import cluster\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.utils import check_random_state, check_symmetric\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.utils import median_survival_times\n",
    "from lifelines.statistics import multivariate_logrank_test\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "class MLP(nn.Layer):\n",
    "    def __init__(self, in_dim, out_dim, hid_dim):\n",
    "        # hid_dim should be a list\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.LayerList()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        # 1st layer\n",
    "        self.layers.append(nn.Linear(self.in_dim, self.hid_dim[0], weight_attr=nn.initializer.KaimingUniform()))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        # hidden layer\n",
    "        for i in range(len(self.hid_dim) - 1):\n",
    "            self.layers.append(nn.Linear(self.hid_dim[i], self.hid_dim[i+1], weight_attr=nn.initializer.KaimingUniform()))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        # last layer\n",
    "        self.out_layer = nn.Linear(self.hid_dim[-1], out_dim, weight_attr=nn.initializer.KaimingUniform())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h = layer(h)\n",
    "        h = self.out_layer(h)\n",
    "        h = paddle.tanh_(h)\n",
    "        return h\n",
    "\n",
    "class AdoptiveSoftThreshold(nn.Layer):\n",
    "    def __init__(self, dim):\n",
    "        super(AdoptiveSoftThreshold, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.add_parameter(\"bias\", paddle.create_parameter(shape=[self.dim], dtype=\"float32\", attr=nn.initializer.Constant(value=0.0)))\n",
    "    \n",
    "    def forward(self, c):\n",
    "        return paddle.sign(c) * F.relu(paddle.abs(c) - self.bias)\n",
    "\n",
    "\n",
    "class SENet(nn.Layer):\n",
    "    def __init__(self, in_dim, out_dim, hid_dim):\n",
    "        super(SENet, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        # layers\n",
    "        self.query_net = MLP(in_dim=self.in_dim, out_dim=self.out_dim, hid_dim=self.hid_dim)\n",
    "        self.key_net = MLP(in_dim=self.in_dim, out_dim=self.out_dim, hid_dim=self.hid_dim)\n",
    "        self.threshold = AdoptiveSoftThreshold(1)\n",
    "        # hyparameters\n",
    "        self.shrink = 1.0 / out_dim\n",
    "\n",
    "    def query_embedding(self, query):\n",
    "        query_emb = self.query_net(query)\n",
    "        return query_emb\n",
    "    \n",
    "    def key_embedding(self, key):\n",
    "        key_emb = self.key_net(key)\n",
    "        return key_emb\n",
    "\n",
    "    def get_coef(self, query, keys):\n",
    "        c = self.threshold(paddle.mm(query, keys.T))\n",
    "        return self.shrink * c\n",
    "\n",
    "    def forward(self, x, others):\n",
    "        query = self.query_embedding(x)\n",
    "        key = self.key_embedding(others)\n",
    "        out = self.get_coef(query, key)\n",
    "        return out\n",
    "\n",
    "def make_graph(array, k=20):\n",
    "    feature = paddle.to_tensor(array, dtype=\"float32\")\n",
    "    _, idx = paddle.topk(feature, k)\n",
    "    idx = idx.numpy()\n",
    "    edges = list()\n",
    "    for i in range(idx.shape[0]):\n",
    "        for j in idx[i]:\n",
    "            edges.append((i, j))\n",
    "    g = pgl.Graph(edges = edges,\n",
    "                num_nodes = feature.shape[0],\n",
    "                node_feat = {'nfeat':feature.T})\n",
    "    return g.tensor(), feature\n",
    "\n",
    "def regularizer(c, lmbd=1.0):\n",
    "    return lmbd * paddle.sum(paddle.abs(c)) + (1.0 - lmbd) / 2.0 * paddle.sum(paddle.pow(c, 2))\n",
    "\n",
    "def p_normalize(x, p=2):\n",
    "    return x / (paddle.norm(x, p=p, axis=1, keepdim=True) + 1e-6)\n",
    "\n",
    "def get_knn_Aff(C_sparse_normalized, k=3, mode='symmetric'):\n",
    "    C_knn = kneighbors_graph(C_sparse_normalized, k, mode='connectivity', include_self=False, n_jobs=10)\n",
    "    if mode == 'symmetric':\n",
    "        Aff_knn = 0.5 * (C_knn + C_knn.T)\n",
    "    elif mode == 'reciprocal':\n",
    "        Aff_knn = C_knn.multiply(C_knn.T)\n",
    "    else:\n",
    "        raise Exception(\"Mode must be 'symmetric' or 'reciprocal'\")\n",
    "    return Aff_knn\n",
    "\n",
    "def spectral_clustering(affinity_matrix_, n_clusters, k, seed=1, n_init=20):\n",
    "    affinity_matrix_ = check_symmetric(affinity_matrix_)\n",
    "    random_state = check_random_state(seed)\n",
    "\n",
    "    laplacian = sparse.csgraph.laplacian(affinity_matrix_, normed=True)\n",
    "    _, vec = sparse.linalg.eigsh(sparse.identity(laplacian.shape[0]) - laplacian, \n",
    "                                 k=k, sigma=None, which='LA')\n",
    "    embedding = normalize(vec)\n",
    "    _, labels_, _ = cluster.k_means(embedding, n_clusters, \n",
    "                                         random_state=seed, n_init=n_init)\n",
    "    return labels_\n",
    "\n",
    "def get_sparse_rep(model, data, batch_size=10, chunk_size=100, non_zeros = 10000):\n",
    "    N, D = data.shape\n",
    "    non_zeros = min(N, non_zeros)\n",
    "    c = paddle.empty([batch_size, N])\n",
    "    if (N % batch_size != 0):\n",
    "        raise Exception(\"batch_size should be a factor of dataset size.\")\n",
    "    if (N % chunk_size != 0):\n",
    "        raise Exception(\"chunk_size should be a factor of dataset size.\")\n",
    "\n",
    "    val = list()\n",
    "    indicies = list()\n",
    "\n",
    "    with paddle.no_grad():\n",
    "        model.eval()\n",
    "        for i in range(data.shape[0] // batch_size):\n",
    "            chunk = data[i * batch_size:(i + 1) * batch_size].cuda()\n",
    "            q = model.query_embedding(chunk)                                                                                                                                                                 \n",
    "            for j in range(data.shape[0] // chunk_size):\n",
    "                chunk_samples = data[j * chunk_size: (j + 1) * chunk_size].cuda()\n",
    "                k = model.key_embedding(chunk_samples)   \n",
    "                coef = model.get_coef(q, k)\n",
    "                c[:, j * chunk_size:(j + 1) * chunk_size] = coef.cpu()\n",
    "            \n",
    "            # diag c reset to zero\n",
    "            rows = list(range(batch_size))\n",
    "            cols = [j + i * batch_size for j in rows]\n",
    "            c[rows, cols] = 0.0\n",
    "            tmp = paddle.zeros_like(c)\n",
    "            # sort\n",
    "            _, index = paddle.topk(paddle.abs(c), axis=1, k=non_zeros)\n",
    "            for line in range(index.shape[0]):\n",
    "                tmp[line] = c[line].gather(index[line])\n",
    "            # val.append(paddle.gather(c, index=index, axis=1).reshape([-1]).cpu().numpy())\n",
    "            val.append(tmp.reshape([-1]).cpu().numpy())\n",
    "            index = index.reshape([-1]).cpu().numpy()\n",
    "            indicies.append(index)\n",
    "\n",
    "    val = np.concatenate(val, axis=0)\n",
    "    indicies = np.concatenate(indicies, axis=0)\n",
    "    indptr = [non_zeros * i for i in range(N + 1)]\n",
    "    \n",
    "    C_sparse = sparse.csr_matrix((val, indicies, indptr), shape=[N, N])\n",
    "    return C_sparse\n",
    "    \n",
    "def lifeline_analysis(df, n_groups):\n",
    "    kmf = KaplanMeierFitter()\n",
    "    plt.figure()\n",
    "    for group in range(n_groups):\n",
    "        idx = (df[\"label\"] == group)\n",
    "        kmf.fit(df['Survival'][idx], df['Death'][idx], label = group)\n",
    "        ax = kmf.plot()\n",
    "        treatment_median_confidence_interval_ = median_survival_times(kmf.confidence_interval_)\n",
    "\n",
    "def evaluate(senet, data, num_subspaces, spectral_dim, non_zeros=1000, n_neighbors=3,\n",
    "             batch_size=10000, chunk_size=10000, affinity='nearest_neighbor', knn_mode='symmetric'):\n",
    "    C_sparse = get_sparse_rep(model=senet, data=data, batch_size=batch_size,\n",
    "                              chunk_size=chunk_size, non_zeros=non_zeros)\n",
    "    C_sparse_normalized = normalize(C_sparse).astype(np.float32)\n",
    "    # plt.matshow(np.sort(np.abs(C_sparse_normalized.toarray())))\n",
    "    if affinity == 'symmetric':\n",
    "        Aff = 0.5 * (np.abs(C_sparse_normalized) + np.abs(C_sparse_normalized).T)\n",
    "    elif affinity == 'nearest_neighbor':\n",
    "        Aff = get_knn_Aff(C_sparse_normalized, k=n_neighbors, mode=knn_mode)\n",
    "    else:\n",
    "        raise Exception(\"affinity should be 'symmetric' or 'nearest_neighbor'\")\n",
    "    \n",
    "    preds = spectral_clustering(Aff, num_subspaces, spectral_dim)\n",
    "\n",
    "    return C_sparse_normalized, preds\n",
    "\n",
    "def load_data(path):\n",
    "    path = os.path.join(\"TCGA\", path)\n",
    "    exp = pd.read_csv(os.path.join(path, \"exp\"), sep = \" \")\n",
    "    methy = pd.read_csv(os.path.join(path, \"methy\"), sep = \" \")\n",
    "    mirna = pd.read_csv(os.path.join(path, \"mirna\"), sep = \" \")\n",
    "    survival = pd.read_csv(os.path.join(path, \"survival\"), sep = \"\\t\")\n",
    "    survival = survival.dropna(axis=0)\n",
    "    if len(survival[\"PatientID\"][0]) > len('tcga.16.1060'):\n",
    "        name_list = list()\n",
    "        survival[\"PatientID\"] = [re.sub(\"-\", \".\", x) for x in survival[\"PatientID\"].str.upper()]\n",
    "        for token in survival[\"PatientID\"]:\n",
    "            if token[-2] != \"0\":\n",
    "                survival.drop(survival[survival[\"PatientID\"] == token].index, inplace = True)\n",
    "                continue\n",
    "            name_list.append(token)\n",
    "            if token not in exp:\n",
    "                exp[token] = exp.mean(axis=1)\n",
    "            if token not in methy:\n",
    "                methy[token] = methy.mean(axis=1)\n",
    "            if token not in mirna:\n",
    "                mirna[token] = mirna.mean(axis=1)\n",
    "    else:\n",
    "        survival[\"PatientID\"] = [re.sub(\"-\", \".\", x) for x in survival[\"PatientID\"].str.upper() + \".01\"]\n",
    "        for token in survival[\"PatientID\"]:\n",
    "            if token not in exp:\n",
    "                exp[token] = exp.mean(axis=1)\n",
    "            if token not in methy:\n",
    "                methy[token] = methy.mean(axis=1)\n",
    "            if token not in mirna:\n",
    "                mirna[token] = mirna.mean(axis=1)\n",
    "        name_list = survival[\"PatientID\"]\n",
    "    exp = exp[name_list]\n",
    "    methy = methy[name_list]\n",
    "    mirna = mirna[name_list]\n",
    "\n",
    "    return [exp, methy, mirna, survival]\n",
    "\n",
    "def draw_coef(coefficient, pred, N, epoch):\n",
    "    pc = pd.DataFrame(np.abs(coefficient.toarray()))\n",
    "    pc = pc + pc.T\n",
    "    pc[\"label\"] = pred\n",
    "    pc = pc.sort_values(\"label\")\n",
    "    idx = pc.index\n",
    "    pc = pc[idx]\n",
    "    plt.pcolor(pc)\n",
    "    plt.savefig(os.path.join(\"/home/aistudio/.jupyter/lab/workspaces/figures\", str(N), str(epoch), \".png\"))\n",
    "    # plt.show()\n",
    "\n",
    "class GCN(nn.Layer):\n",
    "    \"\"\"Implement of GCN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 num_class,\n",
    "                 num_layers=3,\n",
    "                 hidden_size=256,\n",
    "                 **kwargs):\n",
    "        super(GCN, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gcns = nn.LayerList()\n",
    "        for i in range(self.num_layers):\n",
    "            if i == 0:\n",
    "                self.gcns.append(\n",
    "                    pgl.nn.GCNConv(\n",
    "                        input_size,\n",
    "                        self.hidden_size,\n",
    "                        activation=\"relu\",\n",
    "                        norm=True))\n",
    "            else:\n",
    "                self.gcns.append(\n",
    "                    pgl.nn.GCNConv(\n",
    "                        self.hidden_size,\n",
    "                        self.hidden_size,\n",
    "                        activation=\"relu\",\n",
    "                        norm=True))\n",
    "        self.output = nn.Linear(self.hidden_size, self.num_class, weight_attr=nn.initializer.KaimingUniform())\n",
    "    def forward(self, graph, feature):\n",
    "        for m in self.gcns:\n",
    "            feature = m(graph, feature)\n",
    "        logits = self.output(feature)\n",
    "        return logits\n",
    "\n",
    "class Attention(nn.Layer):\n",
    "    def __init__(self, in_size, hidden_size=16):\n",
    "        super(Attention, self).__init__()\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Linear(in_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        w = self.project(z)\n",
    "        beta = paddle.nn.functional.softmax(w, axis=1)\n",
    "        return (beta * z).sum(1), beta\n",
    "\n",
    "class SEGN(nn.Layer):\n",
    "    def __init__(self, in_dim, out_dim, hid_dim):\n",
    "        super(SEGN, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        # layers\n",
    "        self.special_emb_exp = GCN(input_size=self.in_dim, num_class=self.out_dim, hidden_size=self.hid_dim)\n",
    "        self.special_emb_methy = GCN(input_size=self.in_dim, num_class=self.out_dim, hidden_size=self.hid_dim)\n",
    "        self.special_emb_mirna = GCN(input_size=self.in_dim, num_class=self.out_dim, hidden_size=self.hid_dim)\n",
    "\n",
    "        self.common_emb = GCN(input_size=self.in_dim, num_class=self.out_dim, hidden_size=self.hid_dim)\n",
    "        self.threshold = AdoptiveSoftThreshold(1)\n",
    "\n",
    "        self.attention = Attention(out_dim, hidden_size=hid_dim)\n",
    "        # hyparameters\n",
    "        self.shrink = 1.0 / out_dim\n",
    "\n",
    "    def special_embedding(self, graphs, features):\n",
    "        emb_1 = self.special_emb_exp(graphs[0], features[0])\n",
    "        emb_2 = self.special_emb_methy(graphs[1], features[1])\n",
    "        emb_3 = self.special_emb_mirna(graphs[2], features[2])\n",
    "\n",
    "        return paddle.stack([emb_1, emb_2, emb_3], axis=1)\n",
    "    \n",
    "    def shared_embedding(self, graphs, features):\n",
    "        shared_emb = self.common_emb(graphs, features)\n",
    "        return shared_emb\n",
    "\n",
    "    def get_coef(self, query, keys):\n",
    "        c = self.threshold(paddle.mm(query, keys.T))\n",
    "        return self.shrink * c\n",
    "\n",
    "    def forward(self, graphs, features):\n",
    "        shared_1 = self.shared_embedding(graphs[0], features[0])\n",
    "        shared_2 = self.shared_embedding(graphs[1], features[1])\n",
    "        shared_3 = self.shared_embedding(graphs[2], features[2])\n",
    "        shared = (shared_1 + shared_2 + shared_3) / 3\n",
    "\n",
    "        special = self.special_embedding(graphs, features)\n",
    "        emb, att = self.attention(special)\n",
    "        out = self.get_coef(shared, emb)\n",
    "        return out, shared, emb, att\n",
    "\n",
    "def make_graph(array, k=20):\n",
    "    feature = paddle.to_tensor(array, dtype=\"float32\")\n",
    "    _, idx = paddle.topk(feature, k)\n",
    "    idx = idx.numpy()\n",
    "    edges = list()\n",
    "    for i in range(idx.shape[0]):\n",
    "        for j in idx[i]:\n",
    "            edges.append((i, j))\n",
    "    g = pgl.Graph(edges = edges,\n",
    "                num_nodes = feature.shape[0],\n",
    "                node_feat = {'nfeat':feature.T})\n",
    "    return g.tensor(), feature\n",
    "\n",
    "def g_evaluate(model, graphs, features, num_subspaces, spectral_dim, non_zeros=1000, n_neighbors=3,\n",
    "             batch_size=10000, chunk_size=10000, affinity='nearest_neighbor', knn_mode='symmetric'):\n",
    "    N, D = features[0].shape\n",
    "    non_zeros = min(N, non_zeros)\n",
    "    c = paddle.empty([batch_size, N])\n",
    "    val = list()\n",
    "    indicies = list()\n",
    "\n",
    "    with paddle.no_grad():\n",
    "        model.eval()\n",
    "        coef, shared, emb, att = g_ae(graphs, features)\n",
    "        c = coef.cpu()\n",
    "\n",
    "        rows = list(range(batch_size))\n",
    "        cols = [j for j in rows]\n",
    "        c[rows, cols] = 0.0\n",
    "        tmp = paddle.zeros_like(c)\n",
    "        # sort\n",
    "        _, index = paddle.topk(paddle.abs(c), axis=1, k=batch_size)\n",
    "        for line in range(index.shape[0]):\n",
    "            tmp[line] = c[line].gather(index[line])\n",
    "        # val.append(paddle.gather(c, index=index, axis=1).reshape([-1]).cpu().numpy())\n",
    "        val.append(tmp.reshape([-1]).cpu().numpy())\n",
    "        index = index.reshape([-1]).cpu().numpy()\n",
    "        indicies.append(index)\n",
    "    val = np.concatenate(val, axis=0)\n",
    "    indicies = np.concatenate(indicies, axis=0)\n",
    "    indptr = [non_zeros * i for i in range(N + 1)]\n",
    "    C_sparse = sparse.csr_matrix((val, indicies, indptr), shape=[N, N])\n",
    "    \n",
    "    C_sparse_normalized = normalize(C_sparse).astype(np.float32)\n",
    "    # plt.matshow(np.sort(np.abs(C_sparse_normalized.toarray())))\n",
    "    if affinity == 'symmetric':\n",
    "        Aff = 0.5 * (np.abs(C_sparse_normalized) + np.abs(C_sparse_normalized).T)\n",
    "    elif affinity == 'nearest_neighbor':\n",
    "        Aff = get_knn_Aff(C_sparse_normalized, k=n_neighbors, mode=knn_mode)\n",
    "    else:\n",
    "        raise Exception(\"affinity should be 'symmetric' or 'nearest_neighbor'\")\n",
    "    \n",
    "    preds = spectral_clustering(Aff, num_subspaces, spectral_dim)\n",
    "\n",
    "    return C_sparse_normalized, preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T09:47:16.587869Z",
     "iopub.status.busy": "2022-06-15T09:47:16.587385Z",
     "iopub.status.idle": "2022-06-15T09:47:33.771589Z",
     "shell.execute_reply": "2022-06-15T09:47:33.770774Z",
     "shell.execute_reply.started": "2022-06-15T09:47:16.587841Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cancer_type = \"coad\"\n",
    "[exp, methy, mirna, survival] = load_data(cancer_type)\n",
    "\n",
    "conf = dict()\n",
    "conf[\"dataset\"] = cancer_type\n",
    "conf[\"chunk_size\"] = exp.shape[1]\n",
    "\n",
    "conf[\"batch_size\"] = 100\n",
    "conf[\"out_dim\"] = 512\n",
    "conf[\"hid_dim\"] = [1024, 1024]\n",
    "\n",
    "conf[\"learning_rate\"] = 1e-3\n",
    "conf[\"lmbd\"] = 0.1\n",
    "conf[\"gamma\"] = 50\n",
    "conf[\"min_lr\"] = 1e-4\n",
    "conf[\"total_iters\"] = 20000\n",
    "conf[\"save_iter\"] = 2000\n",
    "conf[\"eval_iter\"] = 500\n",
    "if conf[\"dataset\"] == \"bic\":\n",
    "    conf[\"subspace\"] = 5\n",
    "elif conf[\"dataset\"] == \"coad\":\n",
    "    conf[\"subspace\"] = 4\n",
    "elif conf[\"dataset\"] is \"gbm\":\n",
    "    conf[\"subspace\"] = 3\n",
    "elif conf[\"dataset\"] is \"kirc\":\n",
    "    conf[\"subspace\"] = 4\n",
    "elif conf[\"dataset\"] is \"ov\":\n",
    "    conf[\"subspace\"] = 3\n",
    "elif conf[\"dataset\"] is \"lusc\":\n",
    "    conf[\"subspace\"] = 5\n",
    "elif conf[\"dataset\"] is \"skcm\":\n",
    "    conf[\"subspace\"] = 5\n",
    "else:\n",
    "    pass\n",
    "conf[\"spectral_dim\"] = 15\n",
    "conf[\"non_zeros\"] = 10000\n",
    "conf[\"n_neighbors\"] = 3\n",
    "conf[\"affinity\"] = \"nearest_neighbor\"\n",
    "conf[\"g_lmbd\"] = 0.5\n",
    "conf[\"g_gamma\"] = 10\n",
    "conf[\"g_learning_rate\"] = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-11T08:38:48.033418Z",
     "iopub.status.busy": "2022-06-11T08:38:48.032997Z",
     "iopub.status.idle": "2022-06-11T08:38:48.049915Z",
     "shell.execute_reply": "2022-06-11T08:38:48.049437Z",
     "shell.execute_reply.started": "2022-06-11T08:38:48.033389Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_process(data, conf, which):\n",
    "    row_data = paddle.to_tensor(data.to_numpy(), dtype=\"float32\")\n",
    "    # row_data shape [20531, 1229]\n",
    "    global_step = 0\n",
    "\n",
    "    folder = \"{}_result\".format(conf[\"dataset\"])\n",
    "    if not os.path.exists(folder):\n",
    "        os.mkdir(folder)\n",
    "    result = open(f'{folder}/{which}_results.csv', 'w+')\n",
    "    writer = csv.writer(result)\n",
    "    writer.writerow([\"-log(p)\", \"p\", \"iters\"])\n",
    "    for N in [200]:\n",
    "        block_size = min(N, 600)\n",
    "        sample_idx = np.random.choice(row_data.shape[1], N, replace=False)\n",
    "        data = row_data.T[sample_idx]\n",
    "        data = p_normalize(data)\n",
    "        sample_shape = data.shape\n",
    "        n_iter_per_epoch = data.shape[0] // conf[\"batch_size\"]\n",
    "        # data.shape [sampel, dim]\n",
    "        n_step_per_iter = round(data.shape[0] // block_size)\n",
    "        n_epochs = conf[\"total_iters\"] // n_iter_per_epoch\n",
    "        \n",
    "        local_senet = SENet(sample_shape[1], conf[\"out_dim\"], conf[\"hid_dim\"])\n",
    "\n",
    "        # para_dict = paddle.load(\"local_net.pdparams\")\n",
    "        # local_senet.load_dict(para_dict)\n",
    "        \n",
    "        clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=0.001)\n",
    "        opt = paddle.optimizer.AdamW(learning_rate=conf[\"learning_rate\"], parameters=local_senet.parameters(), grad_clip=clip)\n",
    "        scheduler = paddle.optimizer.lr.CosineAnnealingDecay(conf[\"learning_rate\"], T_max=n_epochs, eta_min=conf[\"min_lr\"])\n",
    "\n",
    "        n_iter = 0\n",
    "        pbar = tqdm(range(n_epochs), ncols=120)\n",
    "        for epoch in pbar:\n",
    "            pbar.set_description(f\"Epoch {epoch}\")\n",
    "            randidx = paddle.randperm(sample_shape[0])\n",
    "            for i in range(n_iter_per_epoch):\n",
    "                # each batch in sample\n",
    "                local_senet.train()\n",
    "                batch_idx = randidx[i * conf[\"batch_size\"] : (i + 1) * conf[\"batch_size\"]]\n",
    "                batch = data[batch_idx]\n",
    "                # process all embedding of query and key\n",
    "                query_batch = local_senet.query_embedding(batch)\n",
    "                key_batch = local_senet.query_embedding(batch)\n",
    "\n",
    "                rec_batch = paddle.zeros_like(batch)\n",
    "                reg = paddle.zeros([1])\n",
    "                # each batch be reconstructed by sample\n",
    "                for j in range(n_step_per_iter):\n",
    "                    block = data[j * block_size: (j + 1) * block_size]\n",
    "                    key_block = local_senet.key_embedding(block)\n",
    "                    coef = local_senet.get_coef(query_batch, key_block)\n",
    "                    rec_batch = rec_batch + paddle.mm(coef, block)\n",
    "                    reg = reg + regularizer(coef, conf[\"lmbd\"])\n",
    "            \n",
    "                diag_c = local_senet.threshold((query_batch * key_batch).sum(axis=1, keepdim=True)) * local_senet.shrink\n",
    "                rec_batch = rec_batch - diag_c * batch\n",
    "                rec_loss = paddle.sum(paddle.pow(batch - rec_batch, 2))\n",
    "                loss = (0.5 * conf[\"gamma\"] * rec_loss + reg) / conf[\"batch_size\"]\n",
    "\n",
    "                opt.clear_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                global_step += 1\n",
    "                n_iter += 1\n",
    "\n",
    "                if n_iter % conf[\"save_iter\"] == 0:\n",
    "                    paddle.save(local_senet.state_dict(), f\"{folder}/{which}/{which}_{n_iter}_local.pdparams\")\n",
    "                    paddle.save(opt.state_dict(), \"opt.pdopt\")\n",
    "                    # print(\"Save Success.\")\n",
    "                \n",
    "                if n_iter % conf[\"eval_iter\"] == 0:\n",
    "                    # print(\"Evaluating on {}-full...\".format(conf[\"dataset\"]))\n",
    "                    full_data = p_normalize(row_data)\n",
    "                    coefficient, pred = evaluate(local_senet, data=full_data.T, num_subspaces=conf[\"subspace\"], affinity=conf[\"affinity\"],\n",
    "                                            spectral_dim=conf[\"spectral_dim\"], non_zeros=conf[\"non_zeros\"], n_neighbors=conf[\"n_neighbors\"], batch_size=conf[\"chunk_size\"],\n",
    "                                            chunk_size=conf[\"chunk_size\"], knn_mode='symmetric')\n",
    "                    survival[\"label\"] = pred\n",
    "                    df = survival\n",
    "                    # lifeline_analysis(df, conf[\"subspace\"])\n",
    "                    results = multivariate_logrank_test(df['Survival'], df['label'], df['Death'])\n",
    "                    if results.summary[\"p\"].item() < 7e-7:\n",
    "                        paddle.save(local_senet.state_dict(), f\"{folder}/{which}/{which}_{n_iter}_local.pdparams\")\n",
    "                    # print(\"-log2(p)-{:.6f}, p-{:.6f}\".format(results.summary[\"-log2(p)\"].item(), results.summary[\"p\"].item()))\n",
    "                    writer.writerow([results.summary[\"-log2(p)\"].item(), results.summary[\"p\"].item(), n_iter])\n",
    "                    # writer.writerow([results.summary[\"-log2(p)\"].item(), results.summary[\"p\"].item(), coefficient.toarray(), pred])\n",
    "                    result.flush()\n",
    "\n",
    "            pbar.set_postfix(loss=\"{:3.4f}\".format(loss.item()),\n",
    "                                rec_loss=\"{:3.4f}\".format(rec_loss.item() / conf[\"batch_size\"]),\n",
    "                                reg=\"{:3.4f}\".format(reg.item() / conf[\"batch_size\"]))\n",
    "            scheduler.step()\n",
    "    \n",
    "    return local_senet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-11T08:39:02.703419Z",
     "iopub.status.busy": "2022-06-11T08:39:02.702652Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0611 16:39:03.784927 38822 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W0611 16:39:03.788691 38822 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n",
      "Epoch 1580:  16%|███▊                    | 1576/10000 [00:32<02:40, 52.52it/s, loss=3.1334, rec_loss=0.1133, reg=0.3017]"
     ]
    }
   ],
   "source": [
    "# start local train\n",
    "exp_net = train_process(exp, conf, \"exp\")\n",
    "methy_net = train_process(methy, conf, \"methy\")\n",
    "mirna_net = train_process(mirna, conf, \"mirna\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-11T08:38:31.765660Z",
     "iopub.status.busy": "2022-06-11T08:38:31.765190Z",
     "iopub.status.idle": "2022-06-11T08:38:31.802340Z",
     "shell.execute_reply": "2022-06-11T08:38:31.801825Z",
     "shell.execute_reply.started": "2022-06-11T08:38:31.765631Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature vis graph by pca\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import load_iris,load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "def draw_distr(c_g, pred_g, c, g, title_g,dim=2):\n",
    "    targert = pred_g\n",
    "    digits = c_g.toarray()\n",
    "\n",
    "    targert_compare = g\n",
    "    digits_compare = c.toarray()\n",
    "    # pca? don't mind\n",
    "    X_tsne = TSNE(n_components=dim,random_state=2048).fit_transform(digits)\n",
    "    X_pca = TSNE(n_components=dim,random_state=2048).fit_transform(digits_compare)\n",
    "\n",
    "    ckpt_dir=\"images\"\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=targert,label=\"AMEM\")\n",
    "    plt.legend()\n",
    "    plt.subplot(122)\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=targert_compare,label=\"subtype-GAN\")\n",
    "    plt.legend()\n",
    "    plt.suptitle(title_g.upper())\n",
    "    plt.savefig('images/digits_tsne-pca.png', dpi=120)\n",
    "    plt.show()\n",
    "\n",
    "def draw_coef(coefficient, pred, who, iters):\n",
    "    pc = pd.DataFrame(np.abs(coefficient.toarray()))\n",
    "    pc = pc + pc.T\n",
    "    pc[\"label\"] = pred\n",
    "    pc = pc.sort_values(\"label\")\n",
    "    idx = pc.index\n",
    "    pc = pc[idx]\n",
    "    plt.pcolor(pc)\n",
    "    plt.savefig(f\"figures/{who}/{iters}\")\n",
    "    # plt.show()\n",
    "\n",
    "def draw_coef_resutl(model, data, who):\n",
    "    for i in range(2000, 22000, 2000):\n",
    "        path = f\"bic_result/{who}/{who}_{str(i)}_local.pdparams\"\n",
    "        para_dict = paddle.load(path)\n",
    "        model.load_dict(para_dict)\n",
    "        c1, pred1 = evaluate(model, data=p_normalize(paddle.to_tensor(data.to_numpy().T, dtype=\"float32\")), num_subspaces=conf[\"subspace\"], affinity=conf[\"affinity\"],\n",
    "                                        spectral_dim=conf[\"spectral_dim\"], non_zeros=conf[\"non_zeros\"], n_neighbors=conf[\"n_neighbors\"], batch_size=conf[\"chunk_size\"],\n",
    "                                        chunk_size=conf[\"chunk_size\"], knn_mode='symmetric')\n",
    "        draw_coef(c1, pred1, str(who), str(i))\n",
    "        \n",
    "def lifeline_analysis(df, n_groups, title_g=\"bic\"):\n",
    "    kmf = KaplanMeierFitter()\n",
    "    plt.figure()\n",
    "    for group in range(n_groups):\n",
    "        idx = (df[\"label\"] == group)\n",
    "        kmf.fit(df['Survival'][idx], df['Death'][idx], label = group)\n",
    "        plt.xlabel(\"lifeline(days)\")\n",
    "        plt.ylabel(\"survival probability\")\n",
    "        ax = kmf.plot()\n",
    "        plt.title(title_g)\n",
    "        treatment_median_confidence_interval_ = median_survival_times(kmf.confidence_interval_)\n",
    "# draw_coef_resutl(mirna_net, mirna, \"mirna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "coef, shared, emb, att = g_ae([g1, g2, g3], [f1, f2,f3])\n",
    "att_pd = pd.DataFrame(att.squeeze(2).numpy())\n",
    "plt.boxplot(att_pd.values, labels=att_pd.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T09:47:33.796535Z",
     "iopub.status.busy": "2022-06-15T09:47:33.796381Z",
     "iopub.status.idle": "2022-06-15T09:47:38.010619Z",
     "shell.execute_reply": "2022-06-15T09:47:38.009994Z",
     "shell.execute_reply.started": "2022-06-15T09:47:33.796515Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0615 17:47:33.808580 11390 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W0615 17:47:33.812252 11390 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n"
     ]
    }
   ],
   "source": [
    "exp_net = SENet(exp.shape[0], conf[\"out_dim\"], conf[\"hid_dim\"])\n",
    "methy_net = SENet(methy.shape[0], conf[\"out_dim\"], conf[\"hid_dim\"])\n",
    "mirna_net = SENet(mirna.shape[0], conf[\"out_dim\"], conf[\"hid_dim\"])\n",
    "if conf[\"dataset\"] is \"bic\":\n",
    "    para_dict = paddle.load(\"bic_result/exp/exp_3000_local.pdparams\") \n",
    "    exp_net.load_dict(para_dict)\n",
    "    para_dict = paddle.load(\"bic_result/methy/methy_14000_local.pdparams\") \n",
    "    methy_net.load_dict(para_dict)\n",
    "    para_dict = paddle.load(\"bic_result/mirna/mirna_16000_local.pdparams\")\n",
    "    mirna_net.load_dict(para_dict)\n",
    "elif conf[\"dataset\"] is \"coad\":\n",
    "    para_dict = paddle.load(\"coad_result/exp/exp_20000_local.pdparams\") \n",
    "    exp_net.load_dict(para_dict)\n",
    "    para_dict = paddle.load(\"coad_result/methy/methy_16000_local.pdparams\") \n",
    "    methy_net.load_dict(para_dict)\n",
    "    para_dict = paddle.load(\"coad_result/mirna/mirna_16000_local.pdparams\")\n",
    "    mirna_net.load_dict(para_dict)\n",
    "elif conf[\"dataset\"] is \"gbm\":\n",
    "    para_dict = paddle.load(\"gbm_result/exp/exp_20000_local.pdparams\") \n",
    "    exp_net.load_dict(para_dict)\n",
    "    para_dict = paddle.load(\"gbm_result/methy/methy_20000_local.pdparams\") \n",
    "    methy_net.load_dict(para_dict)\n",
    "    para_dict = paddle.load(\"gbm_result/mirna/mirna_18000_local.pdparams\")\n",
    "    mirna_net.load_dict(para_dict)\n",
    "elif conf[\"dataset\"] is \"kirc\":\n",
    "    para_dict = paddle.load(\"kirc_result/exp/exp_7500_local.pdparams\") \n",
    "    exp_net.load_dict(para_dict)\n",
    "    para_dict = paddle.load(\"kirc_result/methy/methy_14000_local.pdparams\") \n",
    "    methy_net.load_dict(para_dict)\n",
    "    para_dict = paddle.load(\"kirc_result/mirna/mirna_16000_local.pdparams\")\n",
    "    mirna_net.load_dict(para_dict)\n",
    "elif conf[\"dataset\"] is \"lusc\":\n",
    "    para_dict = paddle.load(\"lusc_result/exp/exp_20000_local.pdparams\") \n",
    "    exp_net.load_dict(para_dict)\n",
    "    para_dict = paddle.load(\"lusc_result/methy/methy_20000_local.pdparams\") \n",
    "    methy_net.load_dict(para_dict)\n",
    "    para_dict = paddle.load(\"lusc_result/mirna/mirna_20000_local.pdparams\")\n",
    "    mirna_net.load_dict(para_dict)\n",
    "elif conf[\"dataset\"] is \"skcm\":\n",
    "    para_dict = paddle.load(\"lusc_result/exp/exp_4000_local.pdparams\") \n",
    "    exp_net.load_dict(para_dict)\n",
    "    para_dict = paddle.load(\"lusc_result/methy/methy_14000_local.pdparams\") \n",
    "    methy_net.load_dict(para_dict)\n",
    "    para_dict = paddle.load(\"lusc_result/mirna/mirna_16000_local.pdparams\")\n",
    "    mirna_net.load_dict(para_dict)\n",
    "c1, pred1 = evaluate(exp_net, data=p_normalize(paddle.to_tensor(exp.to_numpy().T, dtype=\"float32\")), num_subspaces=conf[\"subspace\"], affinity=conf[\"affinity\"],\n",
    "                                        spectral_dim=conf[\"spectral_dim\"], non_zeros=conf[\"non_zeros\"], n_neighbors=conf[\"n_neighbors\"], batch_size=conf[\"chunk_size\"],\n",
    "                                        chunk_size=conf[\"chunk_size\"], knn_mode='symmetric')\n",
    "c2, pred2 = evaluate(methy_net, data=p_normalize(paddle.to_tensor(methy.to_numpy().T, dtype=\"float32\")), num_subspaces=conf[\"subspace\"], affinity=conf[\"affinity\"],\n",
    "                                        spectral_dim=conf[\"spectral_dim\"], non_zeros=conf[\"non_zeros\"], n_neighbors=conf[\"n_neighbors\"], batch_size=conf[\"chunk_size\"],\n",
    "                                        chunk_size=conf[\"chunk_size\"], knn_mode='symmetric')\n",
    "c3, pred3 = evaluate(mirna_net, data=p_normalize(paddle.to_tensor(mirna.to_numpy().T, dtype=\"float32\")), num_subspaces=conf[\"subspace\"], affinity=conf[\"affinity\"],\n",
    "                                        spectral_dim=conf[\"spectral_dim\"], non_zeros=conf[\"non_zeros\"], n_neighbors=conf[\"n_neighbors\"], batch_size=conf[\"chunk_size\"],\n",
    "                                        chunk_size=conf[\"chunk_size\"], knn_mode='symmetric')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T09:40:14.431167Z",
     "iopub.status.busy": "2022-06-10T09:40:14.430826Z",
     "iopub.status.idle": "2022-06-10T09:40:14.439114Z",
     "shell.execute_reply": "2022-06-10T09:40:14.438362Z",
     "shell.execute_reply.started": "2022-06-10T09:40:14.431138Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "label = pd.DataFrame(pred1, columns=[\"cluster\"])\n",
    "label.to_csv(f\"data/{conf['dataset']}_pred.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T09:47:55.141580Z",
     "iopub.status.busy": "2022-06-15T09:47:55.141352Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|███████▍                          | 17585/80000 [06:04<20:39, 50.36it/s, loss=22.6659, rec_loss=0.3898, reg=1.1943]"
     ]
    }
   ],
   "source": [
    "conf[\"g_lmbd\"] = 0.3\n",
    "conf[\"g_gamma\"] = 100\n",
    "conf[\"g_learning_rate\"] = 1e-3\n",
    "\n",
    "folder = \"{}_result\".format(conf[\"dataset\"])\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "result = open(f'{folder}/graph_results.csv', 'w+')\n",
    "writer = csv.writer(result)\n",
    "writer.writerow([\"-log(p)\", \"p\", \"iters\"])\n",
    "\n",
    "\n",
    "# get graph and normalized\n",
    "if conf[\"dataset\"] is \"bic\":\n",
    "    # 4000\n",
    "    g1, f1 = make_graph(c1.toarray(), 12)\n",
    "    g2, f2 = make_graph(c2.toarray(), 12)\n",
    "    g3, f3 = make_graph(c3.toarray(), 12)\n",
    "elif conf[\"dataset\"] is \"coad\":\n",
    "    # 12000\n",
    "    g1, f1 = make_graph(c1.toarray(), 4)\n",
    "    g2, f2 = make_graph(c2.toarray(), 4)\n",
    "    g3, f3 = make_graph(c3.toarray(), 4)\n",
    "elif conf[\"dataset\"] is \"gbm\":\n",
    "    # 12000\n",
    "    g1, f1 = make_graph(c1.toarray(), 5)\n",
    "    g2, f2 = make_graph(c2.toarray(), 5)\n",
    "    g3, f3 = make_graph(c3.toarray(), 5)\n",
    "elif conf[\"dataset\"] is \"kirc\":\n",
    "    # 12000\n",
    "    g1, f1 = make_graph(c1.toarray(), 6)\n",
    "    g2, f2 = make_graph(c2.toarray(), 6)\n",
    "    g3, f3 = make_graph(c3.toarray(), 6)\n",
    "elif conf[\"dataset\"] is \"lusc\":\n",
    "    g1, f1 = make_graph(c1.toarray(), 6)\n",
    "    g2, f2 = make_graph(c2.toarray(), 5)\n",
    "    g3, f3 = make_graph(c3.toarray(), 5)\n",
    "elif conf[\"dataset\"] is \"kirc\":\n",
    "    g1, f1 = make_graph(c1.toarray(), 5)\n",
    "    g2, f2 = make_graph(c2.toarray(), 5)\n",
    "    g3, f3 = make_graph(c3.toarray(), 5)\n",
    "f1 = p_normalize(f1)\n",
    "f2 = p_normalize(f2)\n",
    "f3 = p_normalize(f3)\n",
    "g_dim = [f1.shape[0], f1.shape[0], 512]\n",
    "g_ae = SEGN(g_dim[0], g_dim[1], g_dim[2])\n",
    "\n",
    "\n",
    "clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=0.001)\n",
    "g_opt = paddle.optimizer.AdamW(learning_rate=conf[\"g_learning_rate\"], parameters=g_ae.parameters(), grad_clip=clip)\n",
    "pbar = tqdm(range(80000), ncols=120)\n",
    "\n",
    "n_iter = 0\n",
    "\n",
    "for epoch in pbar:\n",
    "    g_ae.train()\n",
    "    coef, shared, emb, att = g_ae([g1, g2, g3], [f1, f2,f3])\n",
    "    diag_c = g_ae.threshold((shared * emb).sum(axis=1, keepdim=True)) * g_ae.shrink\n",
    "    rec_g1 = paddle.mm(coef, f1) - diag_c * f1\n",
    "    rec_g2 = paddle.mm(coef, f2) - diag_c * f2\n",
    "    rec_g3 = paddle.mm(coef, f3) - diag_c * f3\n",
    "\n",
    "    reg = regularizer(coef, conf[\"g_lmbd\"])\n",
    "\n",
    "    g_loss1 = (0.5 * conf[\"g_gamma\"] * paddle.sum(paddle.pow(rec_g1 - f1, 2)) + reg) / f1.shape[0]\n",
    "    g_loss2 = (0.5 * conf[\"g_gamma\"] * paddle.sum(paddle.pow(rec_g2 - f2, 2)) + reg) / f1.shape[0]\n",
    "    g_loss3 = (0.5 * conf[\"g_gamma\"] * paddle.sum(paddle.pow(rec_g3 - f3, 2)) + reg) / f1.shape[0]\n",
    "\n",
    "    rec_loss = paddle.sum(paddle.pow(rec_g1 - f1, 2)) + paddle.sum(paddle.pow(rec_g2 - f2, 2)) + paddle.sum(paddle.pow(rec_g3 - f3, 2))\n",
    "    # loss = g_loss1 + g_loss2 + g_loss3\n",
    "    loss = (0.5 * conf[\"g_gamma\"] * rec_loss + reg) / f1.shape[0]\n",
    "\n",
    "    consis_loss = 0.1 * paddle.sum(paddle.pow(paddle.matmul(shared.T, shared) - paddle.matmul(emb.T, emb) ,2)) / f1.shape[0]\n",
    "    loss += consis_loss \n",
    "    # print(consis_loss.numpy())\n",
    "    # input()\n",
    "\n",
    "    g_opt.clear_grad()\n",
    "    loss.backward()\n",
    "    g_opt.step()\n",
    "    \n",
    "    n_iter += 1\n",
    "\n",
    "    if n_iter % conf[\"save_iter\"] == 0:\n",
    "        plt.figure()\n",
    "        paddle.save(g_ae.state_dict(), f\"{conf['dataset']}_result/global/global_{n_iter}_local.pdparams\")\n",
    "        paddle.save(g_opt.state_dict(), \"g_opt.pdopt\")\n",
    "        # att_pd = pd.DataFrame(att.squeeze(2).numpy())\n",
    "        # plt.boxplot(att_pd.values, labels=att_pd.columns)\n",
    "        # plt.show()\n",
    "        # print(\"Save Success.\")\n",
    "                \n",
    "    if n_iter % conf[\"eval_iter\"] == 0:\n",
    "        # print(\"Evaluating on {}-full...\".format(conf[\"dataset\"]))\n",
    "        coefficient, pred = g_evaluate(g_ae, [g1, g2, g3], [f1, f2,f3], num_subspaces=conf[\"subspace\"], affinity=conf[\"affinity\"],\n",
    "                                        spectral_dim=conf[\"spectral_dim\"], non_zeros=conf[\"non_zeros\"], n_neighbors=conf[\"n_neighbors\"], batch_size=conf[\"chunk_size\"],\n",
    "                                        chunk_size=conf[\"chunk_size\"], knn_mode='symmetric')\n",
    "        survival[\"label\"] = pred\n",
    "        df = survival\n",
    "        # lifeline_analysis(df, conf[\"subspace\"])\n",
    "        results = multivariate_logrank_test(df['Survival'], df['label'], df['Death'])\n",
    "        if results.summary[\"p\"].item() < 0.0003:\n",
    "             paddle.save(g_ae.state_dict(), f\"{conf['dataset']}_result/global/global_{n_iter}_local.pdparams\")\n",
    "        # print(\"-log2(p)-{:.6f}, p-{:.6f}\".format(results.summary[\"-log2(p)\"].item(), results.summary[\"p\"].item()))\n",
    "        writer.writerow([results.summary[\"-log2(p)\"].item(), results.summary[\"p\"].item(), n_iter])\n",
    "        # writer.writerow([results.summary[\"-log2(p)\"].item(), results.summary[\"p\"].item(), coefficient.toarray(), pred])\n",
    "        result.flush()\n",
    "\n",
    "    pbar.set_postfix(loss=\"{:3.4f}\".format(loss.item()),\n",
    "                                rec_loss=\"{:3.4f}\".format(rec_loss.item() / f1.shape[0]),\n",
    "                                reg=\"{:3.4f}\".format(reg.item() / f1.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-12T06:36:19.340113Z",
     "iopub.status.busy": "2022-06-12T06:36:19.339229Z",
     "iopub.status.idle": "2022-06-12T06:36:19.354896Z",
     "shell.execute_reply": "2022-06-12T06:36:19.354313Z",
     "shell.execute_reply.started": "2022-06-12T06:36:19.340070Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "which = \"bic\"\n",
    "exp = pd.read_csv(f\"{which}_result/exp_results.csv\")\n",
    "methy = pd.read_csv(f\"{which}_result/methy_results.csv\")\n",
    "mirna = pd.read_csv(f\"{which}_result/mirna_results.csv\")\n",
    "whole = pd.read_csv(f\"{which}_result/graph_results.csv\")\n",
    "tips =pd.concat([exp[\"-log(p)\"], methy[\"-log(p)\"], mirna[\"-log(p)\"], whole[\"-log(p)\"]], axis=1)\n",
    "tips.columns = [\"mRNA\", \"Methylation\", \"miRNA\", \"AMEM\"]\n",
    "tips[\"dataset\"] = which\n",
    "tips_list.append(tips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-16T02:07:01.216112Z",
     "iopub.status.busy": "2022-04-16T02:07:01.215502Z",
     "iopub.status.idle": "2022-04-16T02:07:01.304143Z",
     "shell.execute_reply": "2022-04-16T02:07:01.303308Z",
     "shell.execute_reply.started": "2022-04-16T02:07:01.216072Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t_0</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>null_distribution</th>\n",
       "      <td>chi squared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degrees_of_freedom</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_name</th>\n",
       "      <td>multivariate_logrank_test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_statistic</th>\n",
       "      <th>p</th>\n",
       "      <th>-log2(p)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.80</td>\n",
       "      <td>0.07</td>\n",
       "      <td>3.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrr}\n",
       "\\toprule\n",
       "{} &  test\\_statistic &         p &  -log2(p) \\\\\n",
       "\\midrule\n",
       "0 &        8.804591 &  0.066174 &  3.917597 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "<lifelines.StatisticalResult: multivariate_logrank_test>\n",
       "               t_0 = -1\n",
       " null_distribution = chi squared\n",
       "degrees_of_freedom = 4\n",
       "         test_name = multivariate_logrank_test\n",
       "\n",
       "---\n",
       " test_statistic    p  -log2(p)\n",
       "           8.80 0.07      3.92"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lifelines.datasets import load_waltons\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.utils import median_survival_times\n",
    "from lifelines.statistics import logrank_test\n",
    "from lifelines.statistics import multivariate_logrank_test\n",
    "\n",
    "test_pd = survival\n",
    "test_pd[\"label_1\"] = pred1\n",
    "test_pd[\"label_2\"] = pred2\n",
    "test_pd[\"label_3\"] = pred3\n",
    "\n",
    "df = test_pd\n",
    "results = multivariate_logrank_test(df['Survival'], df['label_2'], df['Death'])\n",
    "results.print_summary()\n",
    "\n",
    "# df = test_pd\n",
    "# ix = df['label_1'] == 0\n",
    "# T_exp, E_exp = df.loc[ix, 'Survival'], df.loc[ix, 'Death']\n",
    "# T_con, E_con = df.loc[~ix, 'Survival'], df.loc[~ix, 'Death']\n",
    "# results = logrank_test(T_exp, T_con, event_observed_A=E_exp, event_observed_B=E_con)\n",
    "# results.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_shape = train_data.data[0].shape\n",
    "model_header = LocalHeader()\n",
    "model_header.train()\n",
    "\n",
    "epoch = 2000\n",
    "learning_rate = 0.01\n",
    "opt = paddle.optimizer.Adam(learning_rate=learning_rate,parameters=model_header.parameters())\n",
    "\n",
    "x = paddle.to_tensor(train_data.data[0].to_numpy(), dtype=\"float32\").unsqueeze([0,1])\n",
    "\n",
    "for i in tqdm.tqdm(range(epoch)):\n",
    "    # input : ncl\n",
    "    x_reconstructed= model_header(x, True)\n",
    "    # express_loss = paddle.sum(paddle.pow(paddle.subtract(z, z_c), 2.0)) \n",
    "\n",
    "    # coefficient_loss = paddle.sum(paddle.pow(model_header.coefficient, 2)) /1299\n",
    "\n",
    "    # reconstruct_loss = paddle.sum(paddle.pow(paddle.subtract(x, x_reconstructed), 2.0)) / 1299\n",
    "    reconstruct_loss = paddle.nn.functional.mse_loss(input=x_reconstructed, label=x) \n",
    "\n",
    "    if i % 100 ==0:\n",
    "        print(reconstruct_loss)\n",
    "\n",
    "    loss =  reconstruct_loss\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.clear_grad()\n",
    "\n",
    "for i in tqdm.tqdm(range(epoch)):\n",
    "    # input : ncl\n",
    "    x_reconstructed, z, z_c = model_header(x, False)\n",
    "    express_loss = paddle.sum(paddle.pow(paddle.subtract(z, z_c), 2.0)) \n",
    "\n",
    "    # coefficient_loss = paddle.sum(paddle.pow(model_header.coefficient, 2)) /1299\n",
    "\n",
    "    # reconstruct_loss = paddle.sum(paddle.pow(paddle.subtract(x, x_reconstructed), 2.0)) / 1299\n",
    "    reconstruct_loss = paddle.nn.functional.mse_loss(input=x_reconstructed, label=x) \n",
    "\n",
    "    if i % 100 ==0:\n",
    "        print(reconstruct_loss, express_loss)\n",
    "\n",
    "    loss =  reconstruct_loss + express_loss / 12250\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.clear_grad()\n",
    "paddle.save(model_header.state_dict, \"AE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "class LocalHeader(nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(LocalHeader, self).__init__()\n",
    "        # input NCHW\n",
    "        self.conv0 = nn.Conv2D(in_channels=1, out_channels=1, kernel_size=(5, 1))\n",
    "        self.conv1 = nn.Conv2D(in_channels=1, out_channels=1, kernel_size=(5, 1))\n",
    "        self.convT0 = nn.Conv2DTranspose(in_channels=1, out_channels=1, kernel_size=(5, 1))\n",
    "        self.convT1 = nn.Conv2DTranspose(in_channels=1, out_channels=1, kernel_size=(5, 1))\n",
    "\n",
    "        self.coefficient = paddle.create_parameter(shape=[1229, 1229], dtype='float32', default_initializer=paddle.nn.initializer.Uniform(low=0, high=1e-10))\n",
    "        self.add_parameter(\"coef\", self.coefficient)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, is_pretrain=True):\n",
    "        x = self.conv0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        if is_pretrain is False:\n",
    "            # self-expression start\n",
    "            x_shape = x.shape\n",
    "            x_value = x\n",
    "            x = paddle.reshape(x, [-1, 1229])\n",
    "            # x = paddle.matmul(x, self.coefficient - paddle.diag(self.coefficient, padding_value=0))\n",
    "            x = paddle.matmul(x, self.coefficient)\n",
    "            x = paddle.reshape(x, x_shape)\n",
    "            z_value = x\n",
    "            # self-expression end\n",
    "\n",
    "        x = self.convT0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.convT1(x)\n",
    "        x = F.relu(x)\n",
    "        if is_pretrain is False:\n",
    "            return x, x_value, z_value\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-07T06:58:14.494188Z",
     "iopub.status.busy": "2022-06-07T06:58:14.493915Z",
     "iopub.status.idle": "2022-06-07T06:58:14.511068Z",
     "shell.execute_reply": "2022-06-07T06:58:14.510491Z",
     "shell.execute_reply.started": "2022-06-07T06:58:14.494161Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "class SEGraphMaker(nn.Layer):\n",
    "    def __init__(self, num_sample, num_feature, num_cluster):\n",
    "        super(SEGraphMaker, self).__init__()\n",
    "        self.num_sample = num_sample\n",
    "        self.conv0 = paddle.nn.Conv1D(in_channels=1,out_channels=1,kernel_size=5,stride=1)\n",
    "        self.conv1 = paddle.nn.Conv1D(in_channels=1,out_channels=1,kernel_size=5,stride=1)\n",
    "        self.conv2 = paddle.nn.Conv1D(in_channels=1,out_channels=1,kernel_size=5,stride=1)\n",
    "\n",
    "        self.coefficient = paddle.create_parameter(shape=[num_sample, num_sample], dtype='float32', default_initializer=paddle.nn.initializer.Uniform(low=0, high=1e-10))\n",
    "        self.add_parameter(\"coef\", self.coefficient)\n",
    "\n",
    "        self.convT2 = paddle.nn.Conv1DTranspose(in_channels=1,out_channels=1,kernel_size=5,stride=1)\n",
    "        self.convT0 = paddle.nn.Conv1DTranspose(in_channels=1,out_channels=1,kernel_size=5,stride=1)\n",
    "        self.convT1 = paddle.nn.Conv1DTranspose(in_channels=1,out_channels=1,kernel_size=5,stride=1)\n",
    "        \n",
    "\n",
    "        self.bn_encoder = paddle.nn.BatchNorm(2)\n",
    "        self.bn_decoder = paddle.nn.BatchNorm(2)\n",
    "        # self.linear0 = paddle.nn.Linear(num_feature, 256)\n",
    "        # self.linear1 = paddle.nn.Linear(256, num_cluster)\n",
    "\n",
    "    def forward(self, x):\n",
    "        row_x = x\n",
    "        x = self.conv0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        z_reconstruct = self.convT2(x)\n",
    "        z_reconstruct = F.relu(z_reconstruct)\n",
    "        z_reconstruct = self.convT0(z_reconstruct)\n",
    "        z_reconstruct = F.relu(z_reconstruct)\n",
    "        z_reconstruct = self.convT1(z_reconstruct)\n",
    "        z_reconstruct = F.relu(z_reconstruct)\n",
    "\n",
    "        # label = self.linear0(z_reconstruct).squeeze(1)\n",
    "        # label = self.linear1(label)\n",
    "        # label = 0\n",
    "        return z_reconstruct\n",
    "\n",
    "\n",
    "    def thrC(self, C, ro):\n",
    "        C = C.numpy()\n",
    "        if ro < 1:\n",
    "            N = C.shape[1]\n",
    "            Cp = np.zeros((N,N))\n",
    "            S = np.abs(np.sort(-np.abs(C),axis=0))\n",
    "            Ind = np.argsort(-np.abs(C),axis=0)\n",
    "            for i in range(N):\n",
    "                cL1 = np.sum(S[:,i]).astype(float)\n",
    "                stop = False\n",
    "                csum = 0\n",
    "                t = 0\n",
    "                while(stop == False):\n",
    "                    csum = csum + S[t,i]\n",
    "                    if csum > ro*cL1:\n",
    "                        stop = True\n",
    "                        Cp[Ind[0:t+1,i],i] = C[Ind[0:t+1,i],i]\n",
    "                        # print(f\"cp:{Cp}\")\n",
    "                    t = t + 1\n",
    "        else:\n",
    "            Cp = C\n",
    "\n",
    "        return Cp\n",
    "\n",
    "    def post_proC(self, C, K, d, alpha):\n",
    "\n",
    "        # C: coefficient matrix, K: number of clusters, d: dimension of each subspace\n",
    "        C = 0.5*(C + C.T)\n",
    "        r = d*K + 1\n",
    "        U, S, _ = svds(C,r,v0 = np.ones(C.shape[0]))\n",
    "        U = U[:,::-1]    \n",
    "        S = np.sqrt(S[::-1])\n",
    "        S = np.diag(S)    \n",
    "        U = U.dot(S)\n",
    "        U = normalize(U, norm='l2', axis = 1)       \n",
    "        Z = U.dot(U.T)\n",
    "\n",
    "        Z = Z * (Z>0) \n",
    "\n",
    "        L = np.abs(Z ** alpha)\n",
    "        L = L/L.max()\n",
    "        L = 0.5 * (L + L.T)    \n",
    "\n",
    "        spectral = cluster.SpectralClustering(n_clusters=K, eigen_solver='arpack', affinity='precomputed',assign_labels='discretize')\n",
    "        spectral.fit(L)\n",
    "        grp = spectral.fit_predict(L)\n",
    "        return grp, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import tqdm\n",
    "import matplotlib.pyplot  as plt\n",
    "import work.dataloader_tcga as loader\n",
    "import work.model as model\n",
    "import work.graph_maker as maker\n",
    "import work.utils as utils\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# load dataset\n",
    "train_data = loader.TCGADataset(\"bic\", 64)\n",
    "\n",
    "# # get all views and graphs\n",
    "# view_graph = maker.similarityGraph(train_data)\n",
    "# view_origin = utils.parse_view_graph(view_graph)\n",
    "# # inite model\n",
    "# GAESC = model.myModel(1229, [20531, 5000, 1046])\n",
    "\n",
    "# # inite all loss function\n",
    "# ce_loss = paddle.nn.CrossEntropyLoss(reduction='mean', soft_label=True)\n",
    "# # ce_loss = paddle.nn.CrossEntropyLoss(soft_label=True)\n",
    "# mse_loss = paddle.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train part\n",
    "epoch_num = 500\n",
    "history_loss = []\n",
    "iter_epoch = []\n",
    "\n",
    "# GAESC.load_dict(paddle.load(\"model\"))\n",
    "GAESC.train()\n",
    "\n",
    "learning_rate = 0.01\n",
    "opt_AE = paddle.optimizer.Adam(learning_rate=learning_rate,parameters=GAESC.parameters())\n",
    "opt = paddle.optimizer.Adam(learning_rate=learning_rate,parameters=GAESC.parameters())\n",
    "\n",
    "\n",
    "for i in tqdm.tqdm(range(200)):\n",
    "    loss = 0\n",
    "    for index in range(len(view_origin)):\n",
    "        latent, expression = GAESC.train_AE(view_origin[index], index)\n",
    "        pred = paddle.nn.Sigmoid()(paddle.matmul(expression, expression.T))\n",
    "        sexpression_loss = paddle.sum(paddle.pow(paddle.subtract(latent, expression), 2.0))\n",
    "        coef_constraint = paddle.sum(paddle.pow(GAESC.coefficient, 2))\n",
    "        reconstruct_loss = ce_loss(paddle.nn.Sigmoid()(paddle.matmul(expression, expression.T)), view_graph.adj_mat[index])\n",
    "        loss = loss + reconstruct_loss / 1229 + sexpression_loss\n",
    "    loss = loss + coef_constraint\n",
    "    if i %50 ==0:\n",
    "        print(\"loss\", loss , sexpression_loss , coef_constraint, reconstruct_loss)\n",
    "    loss.backward()\n",
    "    opt_AE.step()\n",
    "    opt_AE.clear_grad()\n",
    "\n",
    "for epoch in tqdm.tqdm(range(epoch_num)):\n",
    "    # process all graph from single view\n",
    "    loss = 0\n",
    "    expression_list = list()\n",
    "    if epoch % 100 == 0:\n",
    "        coeff_cluster_label = GAESC.Ncut_clustering()\n",
    "        with open(\"label.txt\", \"a\") as f:\n",
    "            print(coeff_cluster_label, file=f)\n",
    "    for index in range(len(view_origin)):\n",
    "        # calculate all loss, \"Reconstruct Loss\" :cross-entropy loss first\n",
    "        latent, expression, label = GAESC(view_origin[index], index)\n",
    "\n",
    "        # GAESC.coefficient = GAESC.coefficient / paddle.max(paddle.abs(GAESC.coefficient))\n",
    "\n",
    "        reconstruct_loss = ce_loss(paddle.nn.Sigmoid()(paddle.matmul(expression, expression.T)), view_graph.adj_mat[index])\n",
    "        # reconstruct_loss = ce_loss(pretext_label, view_graph.similarityMat[index].astype(\"float32\"))\n",
    "\n",
    "        sexpression_loss = paddle.linalg.norm(paddle.subtract(latent, expression)) ** 2\n",
    "\n",
    "        expression_list.append(expression)\n",
    "\n",
    "        # diag_constraint = paddle.sum(paddle.square(paddle.sum(paddle.abs(GAESC.coefficient), axis=0)))\n",
    "        # diag_constraint = paddle.linalg.norm(GAESC.coefficient) ** 2\n",
    "        diag_constraint = paddle.pow(GAESC.coefficient)\n",
    "\n",
    "        supervision_constraint_part1 = ce_loss(label, paddle.nn.functional.one_hot(paddle.to_tensor(coeff_cluster_label, dtype=\"int32\"), label.shape[1]))\n",
    "        # supervision_constraint_part2 = center_loss()\n",
    "        supervision_constraint = supervision_constraint_part1# + supervision_constraint_part2\n",
    "        supervision_SExpression_loss = utils.supervision_SExpression(coeff_cluster_label, GAESC.coefficient)\n",
    "        loss = loss + reconstruct_loss + sexpression_loss + diag_constraint + supervision_SExpression_loss + supervision_constraint\n",
    "    loss = loss + utils.consistent_constraint(expression_list)\n",
    "    print(\"Loss: \", loss, reconstruct_loss , sexpression_loss , diag_constraint , supervision_SExpression_loss , supervision_constraint)\n",
    "    loss.backward()\n",
    "    iter_epoch.append(epoch)\n",
    "    history_loss.append(loss.numpy()[0])\n",
    "    opt.step()\n",
    "    opt.clear_grad()\n",
    "        \n",
    "plt.plot(iter_epoch,history_loss, label = 'loss')\n",
    "plt.legend()\n",
    "plt.xlabel('iters')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "paddle.save(GAESC.state_dict(),'model')\n",
    "print(\"model saved success.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import paddle\n",
    "from paddle.io import Dataset\n",
    "\n",
    "import tqdm\n",
    "\n",
    "# define a random dataset\n",
    "class myDataset(paddle.io.Dataset):\n",
    "    def __init__(self, data, num_samples):\n",
    "        self.data = data\n",
    "        self.num_samples = num_samples\n",
    "        self.data = paddle.to_tensor(self.transform(data), dtype='float32')\n",
    "\n",
    "    def transform(self, data):\n",
    "        dataset = data.values\n",
    "        return dataset      \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = paddle.unsqueeze(self.data[:, idx], axis=0)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "train_data = myDataset(data[0], data[0].shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "epoch_num = 200\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "def train(train_dataset, batch_size):\n",
    "    print('训练开始')\n",
    "    # 实例化模型\n",
    "    model = AutoEncoder(num_sample=batch_size)\n",
    "    # 将模型转换为训练模式\n",
    "    model.train()\n",
    "    # 设置优化器，学习率，并且把模型参数给优化器\n",
    "    opt = paddle.optimizer.Adam(learning_rate=learning_rate,parameters=model.parameters())\n",
    "    # 设置损失函数\n",
    "    mse_loss = paddle.nn.MSELoss()\n",
    "    # 设置数据读取器\n",
    "    data_reader = paddle.io.DataLoader(train_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        # num_workers=4,\n",
    "                        shuffle=False,\n",
    "                        drop_last=False)\n",
    "    history_loss = []\n",
    "    iter_epoch = []\n",
    "    for epoch in tqdm.tqdm(range(epoch_num)):\n",
    "        for batch_id, data in enumerate(data_reader()):             \n",
    "            x = data\n",
    "            print(data)\n",
    "            out = model(x)\n",
    "            avg_loss = mse_loss(out, x)   \n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "        iter_epoch.append(epoch)\n",
    "        history_loss.append(avg_loss.numpy()[0])\n",
    "    # 绘制loss\n",
    "    plt.plot(iter_epoch,history_loss, label = 'loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('iters')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    # 保存模型参数\n",
    "    paddle.save(model.state_dict(),'model')\n",
    "\n",
    "\n",
    "train(train_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pgl\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "from pgl.utils.logger import log\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "from paddle.optimizer import Adam\n",
    "\n",
    "\n",
    "class GAT(nn.Layer):\n",
    "    \"\"\"Implement of GAT\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            num_class,\n",
    "            num_layers=1,\n",
    "            feat_drop=0.6,\n",
    "            attn_drop=0.6,\n",
    "            num_heads=8,\n",
    "            hidden_size=8, ):\n",
    "        super(GAT, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.num_layers = num_layers\n",
    "        self.feat_drop = feat_drop\n",
    "        self.attn_drop = attn_drop\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gats = nn.LayerList()\n",
    "        for i in range(self.num_layers):\n",
    "            if i == 0:\n",
    "                self.gats.append(\n",
    "                    pgl.nn.GATConv(\n",
    "                        input_size,\n",
    "                        self.hidden_size,\n",
    "                        self.feat_drop,\n",
    "                        self.attn_drop,\n",
    "                        self.num_heads,\n",
    "                        activation='elu'))\n",
    "            elif i == (self.num_layers - 1):\n",
    "                self.gats.append(\n",
    "                    pgl.nn.GATConv(\n",
    "                        self.num_heads * self.hidden_size,\n",
    "                        self.num_class,\n",
    "                        self.feat_drop,\n",
    "                        self.attn_drop,\n",
    "                        1,\n",
    "                        concat=False,\n",
    "                        activation=None))\n",
    "            else:\n",
    "                self.gats.append(\n",
    "                    pgl.nn.GATConv(\n",
    "                        self.num_heads * self.hidden_size,\n",
    "                        self.hidden_size,\n",
    "                        self.feat_drop,\n",
    "                        self.attn_drop,\n",
    "                        self.num_heads,\n",
    "                        activation='elu'))\n",
    "\n",
    "    def forward(self, graph, feature):\n",
    "        for m in self.gats:\n",
    "            feature = m(graph, feature)\n",
    "        print(feature)\n",
    "        return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(feat):\n",
    "    return feat / np.maximum(np.sum(feat, -1, keepdims=True), 1)\n",
    "\n",
    "\n",
    "def load(name, normalized_feature=True):\n",
    "    if name == 'cora':\n",
    "        dataset = pgl.dataset.CoraDataset()\n",
    "    elif name == \"pubmed\":\n",
    "        dataset = pgl.dataset.CitationDataset(\"pubmed\", symmetry_edges=True)\n",
    "    elif name == \"citeseer\":\n",
    "        dataset = pgl.dataset.CitationDataset(\"citeseer\", symmetry_edges=True)\n",
    "    else:\n",
    "        raise ValueError(name + \" dataset doesn't exists\")\n",
    "\n",
    "    indegree = dataset.graph.indegree()\n",
    "    dataset.graph.node_feat[\"words\"] = normalize(dataset.graph.node_feat[\n",
    "        \"words\"])\n",
    "\n",
    "    dataset.graph.tensor()\n",
    "    train_index = dataset.train_index\n",
    "    dataset.train_label = paddle.to_tensor(\n",
    "        np.expand_dims(dataset.y[train_index], -1))\n",
    "    dataset.train_index = paddle.to_tensor(np.expand_dims(train_index, -1))\n",
    "\n",
    "    val_index = dataset.val_index\n",
    "    dataset.val_label = paddle.to_tensor(\n",
    "        np.expand_dims(dataset.y[val_index], -1))\n",
    "    dataset.val_index = paddle.to_tensor(np.expand_dims(val_index, -1))\n",
    "\n",
    "    test_index = dataset.test_index\n",
    "    dataset.test_label = paddle.to_tensor(\n",
    "        np.expand_dims(dataset.y[test_index], -1))\n",
    "    dataset.test_index = paddle.to_tensor(np.expand_dims(test_index, -1))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def train(node_index, node_label, gnn_model, graph, criterion, optim):\n",
    "    gnn_model.train()\n",
    "    pred = gnn_model(graph, graph.node_feat[\"words\"])\n",
    "    pred = paddle.gather(pred, node_index)\n",
    "    loss = criterion(pred, node_label)\n",
    "    loss.backward()\n",
    "    acc = paddle.metric.accuracy(input=pred, label=node_label, k=1)\n",
    "    optim.step()\n",
    "    optim.clear_grad()\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "@paddle.no_grad()\n",
    "def eval(node_index, node_label, gnn_model, graph, criterion):\n",
    "    gnn_model.eval()\n",
    "    pred = gnn_model(graph, graph.node_feat[\"words\"])\n",
    "    pred = paddle.gather(pred, node_index)\n",
    "    loss = criterion(pred, node_label)\n",
    "    acc = paddle.metric.accuracy(input=pred, label=node_label, k=1)\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    paddle.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    dataset = load(args.dataset, args.feature_pre_normalize)\n",
    "\n",
    "    graph = dataset.graph\n",
    "    train_index = dataset.train_index\n",
    "    train_label = dataset.train_label\n",
    "\n",
    "    val_index = dataset.val_index\n",
    "    val_label = dataset.val_label\n",
    "\n",
    "    test_index = dataset.test_index\n",
    "    test_label = dataset.test_label\n",
    "    criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "\n",
    "    dur = []\n",
    "\n",
    "    best_test = []\n",
    "\n",
    "    for run in range(args.runs):\n",
    "        cal_val_acc = []\n",
    "        cal_test_acc = []\n",
    "        cal_val_loss = []\n",
    "        cal_test_loss = []\n",
    "        gnn_model = GAT(input_size=graph.node_feat[\"words\"].shape[1],\n",
    "                        num_class=dataset.num_classes,\n",
    "                        num_layers=2,\n",
    "                        feat_drop=0.6,\n",
    "                        attn_drop=0.6,\n",
    "                        num_heads=8,\n",
    "                        hidden_size=8)\n",
    "\n",
    "        optim = Adam(\n",
    "            learning_rate=0.005,\n",
    "            parameters=gnn_model.parameters(),\n",
    "            weight_decay=0.0005)\n",
    "\n",
    "        for epoch in tqdm.tqdm(range(200)):\n",
    "            if epoch >= 3:\n",
    "                start = time.time()\n",
    "            train_loss, train_acc = train(train_index, train_label, gnn_model,\n",
    "                                          graph, criterion, optim)\n",
    "            if epoch >= 3:\n",
    "                end = time.time()\n",
    "                dur.append(end - start)\n",
    "            val_loss, val_acc = eval(val_index, val_label, gnn_model, graph,\n",
    "                                     criterion)\n",
    "            cal_val_acc.append(val_acc.numpy())\n",
    "            cal_val_loss.append(val_loss.numpy())\n",
    "\n",
    "            test_loss, test_acc = eval(test_index, test_label, gnn_model,\n",
    "                                       graph, criterion)\n",
    "            cal_test_acc.append(test_acc.numpy())\n",
    "            cal_test_loss.append(test_loss.numpy())\n",
    "\n",
    "        log.info(\"Runs %s: Model: GAT Best Test Accuracy: %f\" %\n",
    "                 (run, cal_test_acc[np.argmin(cal_val_loss)]))\n",
    "\n",
    "        best_test.append(cal_test_acc[np.argmin(cal_val_loss)])\n",
    "\n",
    "    log.info(\"Average Speed %s sec/ epoch\" % (np.mean(dur)))\n",
    "    log.info(\"Dataset: %s Best Test Accuracy: %f ( stddev: %f )\" %\n",
    "             (args.dataset, np.mean(best_test), np.std(best_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_GAT(args, graph):\n",
    "\n",
    "    for run in range(args.runs):\n",
    "        cal_val_acc = []\n",
    "        cal_test_acc = []\n",
    "        cal_val_loss = []\n",
    "        cal_test_loss = []\n",
    "        gnn_model = GAT(input_size=graph[\"feature\"].shape[1],\n",
    "                        num_class=dataset.num_classes,\n",
    "                        num_layers=2,\n",
    "                        feat_drop=0.6,\n",
    "                        attn_drop=0.6,\n",
    "                        num_heads=8,\n",
    "                        hidden_size=8)\n",
    "\n",
    "        optim = Adam(\n",
    "            learning_rate=0.005,\n",
    "            parameters=gnn_model.parameters(),\n",
    "            weight_decay=0.0005)\n",
    "\n",
    "        for epoch in tqdm.tqdm(range(200)):\n",
    "            if epoch >= 3:\n",
    "                start = time.time()\n",
    "            train_loss, train_acc = train(train_index, train_label, gnn_model,\n",
    "                                          graph, criterion, optim)\n",
    "            if epoch >= 3:\n",
    "                end = time.time()\n",
    "                dur.append(end - start)\n",
    "            val_loss, val_acc = eval(val_index, val_label, gnn_model, graph,\n",
    "                                     criterion)\n",
    "            cal_val_acc.append(val_acc.numpy())\n",
    "            cal_val_loss.append(val_loss.numpy())\n",
    "\n",
    "            test_loss, test_acc = eval(test_index, test_label, gnn_model,\n",
    "                                       graph, criterion)\n",
    "            cal_test_acc.append(test_acc.numpy())\n",
    "            cal_test_loss.append(test_loss.numpy())\n",
    "\n",
    "        log.info(\"Runs %s: Model: GAT Best Test Accuracy: %f\" %\n",
    "                 (run, cal_test_acc[np.argmin(cal_val_loss)]))\n",
    "\n",
    "        best_test.append(cal_test_acc[np.argmin(cal_val_loss)])\n",
    "\n",
    "    log.info(\"Average Speed %s sec/ epoch\" % (np.mean(dur)))\n",
    "    log.info(\"Dataset: %s Best Test Accuracy: %f ( stddev: %f )\" %\n",
    "             (args.dataset, np.mean(best_test), np.std(best_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_trte_data(data_folder, view_list):\n",
    "    num_view = len(view_list)\n",
    "    # how many omics data to fusion\n",
    "    labels_tr = np.loadtxt(os.path.join(data_folder, \"labels_tr.csv\"), delimiter=',')\n",
    "    labels_te = np.loadtxt(os.path.join(data_folder, \"labels_te.csv\"), delimiter=',')\n",
    "    labels_tr = labels_tr.astype(int)\n",
    "    labels_te = labels_te.astype(int)\n",
    "    data_tr_list = []\n",
    "    data_te_list = []\n",
    "    for i in view_list:\n",
    "        data_tr_list.append(np.loadtxt(os.path.join(data_folder, str(i)+\"_tr.csv\"), delimiter=','))\n",
    "        data_te_list.append(np.loadtxt(os.path.join(data_folder, str(i)+\"_te.csv\"), delimiter=','))\n",
    "    num_tr = data_tr_list[0].shape[0]\n",
    "    num_te = data_te_list[0].shape[0]\n",
    "    data_mat_list = []\n",
    "    for i in range(num_view):\n",
    "        data_mat_list.append(np.concatenate((data_tr_list[i], data_te_list[i]), axis=0))\n",
    "    data_tensor_list = []\n",
    "    for i in range(len(data_mat_list)):\n",
    "        data_tensor_list.append(torch.FloatTensor(data_mat_list[i]))\n",
    "        if cuda:\n",
    "            data_tensor_list[i] = data_tensor_list[i].cuda()\n",
    "    idx_dict = {}\n",
    "    idx_dict[\"tr\"] = list(range(num_tr))\n",
    "    idx_dict[\"te\"] = list(range(num_tr, (num_tr+num_te)))\n",
    "    data_train_list = []\n",
    "    data_all_list = []\n",
    "    for i in range(len(data_tensor_list)):\n",
    "        data_train_list.append(data_tensor_list[i][idx_dict[\"tr\"]].clone())\n",
    "        data_all_list.append(torch.cat((data_tensor_list[i][idx_dict[\"tr\"]].clone(),\n",
    "                                       data_tensor_list[i][idx_dict[\"te\"]].clone()),0))\n",
    "    labels = np.concatenate((labels_tr, labels_te))\n",
    "    \n",
    "    return data_train_list, data_all_list, idx_dict, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"TCGA/BRCA/count_ENSG.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.drop(\"X1\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def thrC(C,ro):\n",
    "    if ro < 1:\n",
    "        N = C.shape[1]\n",
    "        Cp = np.zeros((N,N))\n",
    "        S = np.abs(np.sort(-np.abs(C),axis=0))\n",
    "        \n",
    "        Ind = np.argsort(-np.abs(C),axis=0)\n",
    "\n",
    "        print(f's:{S}, \\nInd:{Ind}')\n",
    "\n",
    "        for i in range(N):\n",
    "            cL1 = np.sum(S[:,i]).astype(float)\n",
    "            stop = False\n",
    "            csum = 0\n",
    "            t = 0\n",
    "            while(stop == False):\n",
    "                csum = csum + S[t,i]\n",
    "                if csum > ro*cL1:\n",
    "                    stop = True\n",
    "                    Cp[Ind[0:t+1,i],i] = C[Ind[0:t+1,i],i]\n",
    "                    # print(f\"cp:{Cp}\")\n",
    "                t = t + 1\n",
    "    else:\n",
    "        Cp = C\n",
    "\n",
    "    return Cp\n",
    "c = np.random.random([10,10])\n",
    "print(c)\n",
    "pro_c = thrC(c, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "from sklearn import cluster\n",
    "from sklearn.preprocessing import normalize\n",
    "def post_proC(C, K, d, alpha):\n",
    "    # C: coefficient matrix, K: number of clusters, d: dimension of each subspace\n",
    "    C = 0.5*(C + C.T)\n",
    "    r = d*K + 1\n",
    "    U, S, _ = svds(C,r,v0 = np.ones(C.shape[0]))\n",
    "    U = U[:,::-1]    \n",
    "    S = np.sqrt(S[::-1])\n",
    "    S = np.diag(S)    \n",
    "    U = U.dot(S)    \n",
    "    U = normalize(U, norm='l2', axis = 1)       \n",
    "    Z = U.dot(U.T)\n",
    "    Z = Z * (Z>0) \n",
    "\n",
    "    L = np.abs(Z ** alpha)\n",
    "    L = L/L.max()\n",
    "    L = 0.5 * (L + L.T)    \n",
    "\n",
    "    spectral = cluster.SpectralClustering(n_clusters=K, eigen_solver='arpack', affinity='precomputed',assign_labels='discretize')\n",
    "    spectral.fit(L)\n",
    "    grp = spectral.fit_predict(L) + 1\n",
    "    return grp, L\n",
    "\n",
    "x, L = post_proC(pro_c, 2, 4, 3.5)\n",
    "import pandas as pd\n",
    "pd.DataFrame(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! python work/dataloader_tcga.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "mirna = pd.read_csv(os.path.join(\"TCGA/bic/\", \"exp\"), sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "class Model(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        saved_tensor = self.create_tensor(name=\"saved_tensor0\")\n",
    "        self.register_buffer(\"saved_tensor\", saved_tensor, persistable=True)\n",
    "        self.flatten = paddle.nn.Flatten()\n",
    "        self.fc = paddle.nn.Linear(10, 100)\n",
    "\n",
    "    def forward(self, input):\n",
    "        y = self.flatten(input)\n",
    "        # Save intermediate tensor\n",
    "        paddle.assign(y, self.saved_tensor)\n",
    "        y = self.fc(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "# paddle.enable_static\n",
    "model = Model()\n",
    "print(model.buffers())\n",
    "for item in model.named_buffers():\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "from paddle.optimizer import Adam\n",
    "import pgl\n",
    "\n",
    "\n",
    "def build_graph():\n",
    "    # define the number of nodes; we can use number to represent every node\n",
    "    num_node = 10\n",
    "    # add edges, we represent all edges as a list of tuple (src, dst)\n",
    "    edge_list = [(2, 0), (2, 1), (3, 1),(4, 0), (5, 0), \n",
    "             (6, 0), (6, 4), (6, 5), (7, 0), (7, 1),\n",
    "             (7, 2), (7, 3), (8, 0), (9, 7)]\n",
    "\n",
    "    # Each node can be represented by a d-dimensional feature vector, here for simple, the feature vectors are randomly generated.\n",
    "    d = 16\n",
    "    feature = np.random.randn(3,num_node, d).astype(\"float32\")\n",
    "    # each edge has it own weight\n",
    "    edge_feature = np.random.randn(len(edge_list), 1).astype(\"float32\")\n",
    "    \n",
    "    # create a graph\n",
    "    g = pgl.Graph(edges = edge_list,\n",
    "                  num_nodes = num_node,\n",
    "                  node_feat = {'nfeat':feature}, \n",
    "                  edge_feat ={'efeat': edge_feature})\n",
    "\n",
    "    return g\n",
    "g = build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T17:00:08.815057Z",
     "iopub.status.busy": "2022-06-08T17:00:08.814580Z",
     "iopub.status.idle": "2022-06-08T17:00:08.857176Z",
     "shell.execute_reply": "2022-06-08T17:00:08.856584Z",
     "shell.execute_reply.started": "2022-06-08T17:00:08.815019Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TCGA.3L.AA1B.01</th>\n",
       "      <th>TCGA.4N.A93T.01</th>\n",
       "      <th>TCGA.4T.AA8H.01</th>\n",
       "      <th>TCGA.5M.AAT4.01</th>\n",
       "      <th>TCGA.5M.AAT6.01</th>\n",
       "      <th>TCGA.5M.AATE.01</th>\n",
       "      <th>TCGA.A6.2670.01</th>\n",
       "      <th>TCGA.A6.2671.01</th>\n",
       "      <th>TCGA.A6.2672.01</th>\n",
       "      <th>TCGA.A6.2674.01</th>\n",
       "      <th>...</th>\n",
       "      <th>TCGA.QG.A5YV.01</th>\n",
       "      <th>TCGA.QG.A5YW.01</th>\n",
       "      <th>TCGA.QG.A5YX.01</th>\n",
       "      <th>TCGA.QG.A5Z1.01</th>\n",
       "      <th>TCGA.QG.A5Z2.01</th>\n",
       "      <th>TCGA.QL.A97D.01</th>\n",
       "      <th>TCGA.RU.A8FL.01</th>\n",
       "      <th>TCGA.SS.A7HO.01</th>\n",
       "      <th>TCGA.T9.A92H.01</th>\n",
       "      <th>TCGA.WS.AB45.01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>?|100130426</th>\n",
       "      <td>0.5174</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.025573</td>\n",
       "      <td>0.025573</td>\n",
       "      <td>0.025573</td>\n",
       "      <td>0.025573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>?|100133144</th>\n",
       "      <td>18.0851</td>\n",
       "      <td>4.4315</td>\n",
       "      <td>9.8995</td>\n",
       "      <td>7.9174</td>\n",
       "      <td>3.9637</td>\n",
       "      <td>11.6290</td>\n",
       "      <td>11.082847</td>\n",
       "      <td>11.082847</td>\n",
       "      <td>11.082847</td>\n",
       "      <td>11.082847</td>\n",
       "      <td>...</td>\n",
       "      <td>11.3825</td>\n",
       "      <td>13.4798</td>\n",
       "      <td>14.7995</td>\n",
       "      <td>14.8599</td>\n",
       "      <td>23.6064</td>\n",
       "      <td>15.6590</td>\n",
       "      <td>15.8447</td>\n",
       "      <td>24.4956</td>\n",
       "      <td>9.3690</td>\n",
       "      <td>3.9827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>?|100134869</th>\n",
       "      <td>15.7640</td>\n",
       "      <td>4.2767</td>\n",
       "      <td>11.3032</td>\n",
       "      <td>18.7608</td>\n",
       "      <td>15.0672</td>\n",
       "      <td>6.9060</td>\n",
       "      <td>8.748315</td>\n",
       "      <td>8.748315</td>\n",
       "      <td>8.748315</td>\n",
       "      <td>8.748315</td>\n",
       "      <td>...</td>\n",
       "      <td>11.8390</td>\n",
       "      <td>12.4813</td>\n",
       "      <td>7.2954</td>\n",
       "      <td>14.7523</td>\n",
       "      <td>8.6184</td>\n",
       "      <td>4.6528</td>\n",
       "      <td>11.8574</td>\n",
       "      <td>16.4102</td>\n",
       "      <td>7.8797</td>\n",
       "      <td>4.4654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>?|10357</th>\n",
       "      <td>144.4004</td>\n",
       "      <td>142.6609</td>\n",
       "      <td>143.1987</td>\n",
       "      <td>146.1876</td>\n",
       "      <td>228.2519</td>\n",
       "      <td>240.7145</td>\n",
       "      <td>218.667527</td>\n",
       "      <td>218.667527</td>\n",
       "      <td>218.667527</td>\n",
       "      <td>218.667527</td>\n",
       "      <td>...</td>\n",
       "      <td>345.5358</td>\n",
       "      <td>144.8068</td>\n",
       "      <td>224.0507</td>\n",
       "      <td>247.3256</td>\n",
       "      <td>136.5238</td>\n",
       "      <td>321.6391</td>\n",
       "      <td>304.8002</td>\n",
       "      <td>263.5938</td>\n",
       "      <td>215.6332</td>\n",
       "      <td>105.0739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>?|10431</th>\n",
       "      <td>774.6615</td>\n",
       "      <td>1185.2927</td>\n",
       "      <td>888.3202</td>\n",
       "      <td>1280.5508</td>\n",
       "      <td>945.6358</td>\n",
       "      <td>606.8490</td>\n",
       "      <td>963.474022</td>\n",
       "      <td>963.474022</td>\n",
       "      <td>963.474022</td>\n",
       "      <td>963.474022</td>\n",
       "      <td>...</td>\n",
       "      <td>961.9207</td>\n",
       "      <td>1296.0559</td>\n",
       "      <td>1061.7840</td>\n",
       "      <td>562.6314</td>\n",
       "      <td>790.1954</td>\n",
       "      <td>964.5725</td>\n",
       "      <td>1007.2661</td>\n",
       "      <td>738.9138</td>\n",
       "      <td>1445.0989</td>\n",
       "      <td>397.4616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZYX|7791</th>\n",
       "      <td>6259.1876</td>\n",
       "      <td>4653.1205</td>\n",
       "      <td>4460.6105</td>\n",
       "      <td>4190.1893</td>\n",
       "      <td>6165.9916</td>\n",
       "      <td>5513.8137</td>\n",
       "      <td>4828.840499</td>\n",
       "      <td>4828.840499</td>\n",
       "      <td>4828.840499</td>\n",
       "      <td>4828.840499</td>\n",
       "      <td>...</td>\n",
       "      <td>3783.1349</td>\n",
       "      <td>4591.9121</td>\n",
       "      <td>2604.3372</td>\n",
       "      <td>4343.2321</td>\n",
       "      <td>3495.7148</td>\n",
       "      <td>4794.5205</td>\n",
       "      <td>4297.0027</td>\n",
       "      <td>1927.7905</td>\n",
       "      <td>4669.3311</td>\n",
       "      <td>8390.9445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZZEF1|23140</th>\n",
       "      <td>1358.3172</td>\n",
       "      <td>1220.1258</td>\n",
       "      <td>3002.0106</td>\n",
       "      <td>1093.3735</td>\n",
       "      <td>1390.5637</td>\n",
       "      <td>733.1615</td>\n",
       "      <td>1791.462566</td>\n",
       "      <td>1791.462566</td>\n",
       "      <td>1791.462566</td>\n",
       "      <td>1791.462566</td>\n",
       "      <td>...</td>\n",
       "      <td>850.1427</td>\n",
       "      <td>933.7993</td>\n",
       "      <td>1227.0867</td>\n",
       "      <td>1233.9531</td>\n",
       "      <td>2500.5142</td>\n",
       "      <td>2176.6651</td>\n",
       "      <td>788.8283</td>\n",
       "      <td>903.8422</td>\n",
       "      <td>974.3374</td>\n",
       "      <td>2163.9127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZZZ3|26009</th>\n",
       "      <td>798.3559</td>\n",
       "      <td>333.8171</td>\n",
       "      <td>530.0676</td>\n",
       "      <td>574.4406</td>\n",
       "      <td>717.2657</td>\n",
       "      <td>586.9411</td>\n",
       "      <td>654.676705</td>\n",
       "      <td>654.676705</td>\n",
       "      <td>654.676705</td>\n",
       "      <td>654.676705</td>\n",
       "      <td>...</td>\n",
       "      <td>613.5983</td>\n",
       "      <td>590.3145</td>\n",
       "      <td>631.3421</td>\n",
       "      <td>1016.2362</td>\n",
       "      <td>796.0233</td>\n",
       "      <td>598.4884</td>\n",
       "      <td>789.7366</td>\n",
       "      <td>688.4345</td>\n",
       "      <td>508.2036</td>\n",
       "      <td>720.4996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psiTPTE22|387590</th>\n",
       "      <td>13.0561</td>\n",
       "      <td>1.9352</td>\n",
       "      <td>2.1934</td>\n",
       "      <td>6.4544</td>\n",
       "      <td>255.9320</td>\n",
       "      <td>5.4918</td>\n",
       "      <td>29.385028</td>\n",
       "      <td>29.385028</td>\n",
       "      <td>29.385028</td>\n",
       "      <td>29.385028</td>\n",
       "      <td>...</td>\n",
       "      <td>3.9358</td>\n",
       "      <td>21.5676</td>\n",
       "      <td>12.2750</td>\n",
       "      <td>5.3840</td>\n",
       "      <td>6.5135</td>\n",
       "      <td>5.1960</td>\n",
       "      <td>2.7248</td>\n",
       "      <td>6.9627</td>\n",
       "      <td>5.0484</td>\n",
       "      <td>80.8601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tAKR|389932</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9676</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4303</td>\n",
       "      <td>0.6562</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.041774</td>\n",
       "      <td>0.041774</td>\n",
       "      <td>0.041774</td>\n",
       "      <td>0.041774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3994</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9083</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20531 rows × 444 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  TCGA.3L.AA1B.01  TCGA.4N.A93T.01  TCGA.4T.AA8H.01  \\\n",
       "?|100130426                0.5174           0.0000           0.0000   \n",
       "?|100133144               18.0851           4.4315           9.8995   \n",
       "?|100134869               15.7640           4.2767          11.3032   \n",
       "?|10357                  144.4004         142.6609         143.1987   \n",
       "?|10431                  774.6615        1185.2927         888.3202   \n",
       "...                           ...              ...              ...   \n",
       "ZYX|7791                6259.1876        4653.1205        4460.6105   \n",
       "ZZEF1|23140             1358.3172        1220.1258        3002.0106   \n",
       "ZZZ3|26009               798.3559         333.8171         530.0676   \n",
       "psiTPTE22|387590          13.0561           1.9352           2.1934   \n",
       "tAKR|389932                0.0000           0.9676           0.0000   \n",
       "\n",
       "                  TCGA.5M.AAT4.01  TCGA.5M.AAT6.01  TCGA.5M.AATE.01  \\\n",
       "?|100130426                0.0000           0.0000           0.0000   \n",
       "?|100133144                7.9174           3.9637          11.6290   \n",
       "?|100134869               18.7608          15.0672           6.9060   \n",
       "?|10357                  146.1876         228.2519         240.7145   \n",
       "?|10431                 1280.5508         945.6358         606.8490   \n",
       "...                           ...              ...              ...   \n",
       "ZYX|7791                4190.1893        6165.9916        5513.8137   \n",
       "ZZEF1|23140             1093.3735        1390.5637         733.1615   \n",
       "ZZZ3|26009               574.4406         717.2657         586.9411   \n",
       "psiTPTE22|387590           6.4544         255.9320           5.4918   \n",
       "tAKR|389932                0.4303           0.6562           0.0000   \n",
       "\n",
       "                  TCGA.A6.2670.01  TCGA.A6.2671.01  TCGA.A6.2672.01  \\\n",
       "?|100130426              0.025573         0.025573         0.025573   \n",
       "?|100133144             11.082847        11.082847        11.082847   \n",
       "?|100134869              8.748315         8.748315         8.748315   \n",
       "?|10357                218.667527       218.667527       218.667527   \n",
       "?|10431                963.474022       963.474022       963.474022   \n",
       "...                           ...              ...              ...   \n",
       "ZYX|7791              4828.840499      4828.840499      4828.840499   \n",
       "ZZEF1|23140           1791.462566      1791.462566      1791.462566   \n",
       "ZZZ3|26009             654.676705       654.676705       654.676705   \n",
       "psiTPTE22|387590        29.385028        29.385028        29.385028   \n",
       "tAKR|389932              0.041774         0.041774         0.041774   \n",
       "\n",
       "                  TCGA.A6.2674.01  ...  TCGA.QG.A5YV.01  TCGA.QG.A5YW.01  \\\n",
       "?|100130426              0.025573  ...           0.0000           0.0000   \n",
       "?|100133144             11.082847  ...          11.3825          13.4798   \n",
       "?|100134869              8.748315  ...          11.8390          12.4813   \n",
       "?|10357                218.667527  ...         345.5358         144.8068   \n",
       "?|10431                963.474022  ...         961.9207        1296.0559   \n",
       "...                           ...  ...              ...              ...   \n",
       "ZYX|7791              4828.840499  ...        3783.1349        4591.9121   \n",
       "ZZEF1|23140           1791.462566  ...         850.1427         933.7993   \n",
       "ZZZ3|26009             654.676705  ...         613.5983         590.3145   \n",
       "psiTPTE22|387590        29.385028  ...           3.9358          21.5676   \n",
       "tAKR|389932              0.041774  ...           0.0000           0.3994   \n",
       "\n",
       "                  TCGA.QG.A5YX.01  TCGA.QG.A5Z1.01  TCGA.QG.A5Z2.01  \\\n",
       "?|100130426                0.0000           0.0000           0.0000   \n",
       "?|100133144               14.7995          14.8599          23.6064   \n",
       "?|100134869                7.2954          14.7523           8.6184   \n",
       "?|10357                  224.0507         247.3256         136.5238   \n",
       "?|10431                 1061.7840         562.6314         790.1954   \n",
       "...                           ...              ...              ...   \n",
       "ZYX|7791                2604.3372        4343.2321        3495.7148   \n",
       "ZZEF1|23140             1227.0867        1233.9531        2500.5142   \n",
       "ZZZ3|26009               631.3421        1016.2362         796.0233   \n",
       "psiTPTE22|387590          12.2750           5.3840           6.5135   \n",
       "tAKR|389932                0.0000           0.0000           0.0000   \n",
       "\n",
       "                  TCGA.QL.A97D.01  TCGA.RU.A8FL.01  TCGA.SS.A7HO.01  \\\n",
       "?|100130426                0.0000           0.0000           0.0000   \n",
       "?|100133144               15.6590          15.8447          24.4956   \n",
       "?|100134869                4.6528          11.8574          16.4102   \n",
       "?|10357                  321.6391         304.8002         263.5938   \n",
       "?|10431                  964.5725        1007.2661         738.9138   \n",
       "...                           ...              ...              ...   \n",
       "ZYX|7791                4794.5205        4297.0027        1927.7905   \n",
       "ZZEF1|23140             2176.6651         788.8283         903.8422   \n",
       "ZZZ3|26009               598.4884         789.7366         688.4345   \n",
       "psiTPTE22|387590           5.1960           2.7248           6.9627   \n",
       "tAKR|389932                0.0000           0.9083           0.0000   \n",
       "\n",
       "                  TCGA.T9.A92H.01  TCGA.WS.AB45.01  \n",
       "?|100130426                0.0000           0.0000  \n",
       "?|100133144                9.3690           3.9827  \n",
       "?|100134869                7.8797           4.4654  \n",
       "?|10357                  215.6332         105.0739  \n",
       "?|10431                 1445.0989         397.4616  \n",
       "...                           ...              ...  \n",
       "ZYX|7791                4669.3311        8390.9445  \n",
       "ZZEF1|23140              974.3374        2163.9127  \n",
       "ZZZ3|26009               508.2036         720.4996  \n",
       "psiTPTE22|387590           5.0484          80.8601  \n",
       "tAKR|389932                0.0000           0.0000  \n",
       "\n",
       "[20531 rows x 444 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
